[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome! :D\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "No-plan Pantry\n\n\n\n\n\n\nwebscraping\n\n\nmachine learning\n\n\nweb development\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nJade Liang, Junying Li, Karis Choi\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting Fake vs. Real News\n\n\n\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Development with Dash\n\n\n\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Databases, Query Functions, and Interactive Plots\n\n\n\n\n\n\npic16b homework\n\n\nvisualizations\n\n\ndatabases\n\n\ninteractive graphics\n\n\nquery\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Construct a Visuaization of the Palmer Penguins Data Set\n\n\n\n\n\n\npic16B homework\n\n\ntutorial\n\n\nvisualizations\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nJade Liang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "Hello world"
  },
  {
    "objectID": "posts/constructing-visualization-of-penguin-data/index.html",
    "href": "posts/constructing-visualization-of-penguin-data/index.html",
    "title": "How to Construct a Visuaization of the Palmer Penguins Data Set",
    "section": "",
    "text": "1. Load the Data Set\nBefore we can create visualizations for the Palmer Penguins Data set, we need to first import the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nHere’s the first five rows of the Palmer Penguins data set:\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n2. Import the Seaborn Package and Create a Visualization\nAfter importing the dataset, we can then import the seaborn package. Then, use seaborn.relplot() from the seaborn package to create a scatter plot that compares the body mass (g) to the flipper length (mm) of each penguin for each sex. Notice that there is a 3rd parameter for Sex where Sex = \".\". This is because there is one entry in the Palmer Penguins data set where the sex of that penguin isn’t specified.\n\nimport seaborn as sns\n\nfgrid = sns.relplot(x = \"Body Mass (g)\", \n                    y = \"Flipper Length (mm)\",\n                    hue = \"Sex\", # to color each point by Sex\n                    data = penguins\n                    )\n\nfgrid.fig.suptitle(\"Body Mass (g) vs. Flipper Length (mm)\") # to add title to plot\nfgrid.fig.subplots_adjust(top=0.9) # to adjust placement of title\n\n\n\n\n\n\n\n\nAnd that’s how you can create a simple scatter plot using Seaborn! You can adjust the arguments of sns.relplot() to create different scatter plots using Palmer Penguins data set."
  },
  {
    "objectID": "posts/hw1_data-wrangling-and-visualization/index.html",
    "href": "posts/hw1_data-wrangling-and-visualization/index.html",
    "title": "Creating Databases, Query Functions, and Interactive Plots",
    "section": "",
    "text": "Welcome! In this blog post, I’ll show you how to create a database, write a query function to access data from tables in a database, and create interesting visualizations using Plotly Express.\n\n1. Create a Database\nFirst, we’ll import the sqlite3 and pandas packages. Then, using sqlite3.connect(), we’ll create a database in our current directory called temps.db.\n\n# importing necessary package\nimport sqlite3\nimport pandas as pd\n\n# to create a database in the current directory called temps.db\nconn = sqlite3.connect(\"temps.db\") \n\nWe’ll be creating a new folder named datafiles, then downloading the temperatures datas into our new folder.\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files for the `temperatures` table\nimport urllib.request\nintervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nNext, we’ll upload the tables temperatures, stations, and countries into our database. Note that there are some NaN values in the temperatures dataset. I’ve removed them using the prepare_df() function. Here’s what the prepare_df() looks like:\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n\n    # removing rows where Temp == NaN\n    df = df.dropna(subset = \"Temp\")\n\n    return df\n\n\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df) # prepares temperatures df and removes nan values\n    df.to_sql(\"temperatures\", conn, \n              if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nurl = \"https://raw.githubusercontent.com/PIC16B-ucla/24F/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\nLet’s check if our tables have been successfully uploaded into our database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nLooks good! Now, after we’re done uploading tables to our database, let’s close our connection to our database using conn.close(). It’s generally a good practice to do this.\n\nconn.close()\n\n\n\n2. Write a Query Function\nNow that we have our tables in the database, let’s create a query function to allow easy access to our data in the database. I’ve created the function in… Here’s what the function looks like:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month) :\n    \"\"\" Extracts climate data from a specified database based on the provided country, \n        year range, and month; and returns extracted data in a Pandas dataframe\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n\n    Returns:\n        df (Pandas dataframe): a dataframe of temperature readings of inputted country according to \n            inputted year_begin, inputted year_end, and inputted month of the year. The\n            resulting dataframe contains the columns `NAME`, 'LATITUDE', 'LONGITUDE`, \n            `Country`, `Year`, `Month`, `Temp`.\n    \"\"\"\n\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(\"\"\"\n                    SELECT s.*, c.Name AS Country, t.*\n                    FROM stations s\n                    INNER JOIN countries c\n                    ON c.\"FIPS 10-4\" = SUBSTR(s.ID, 1, 2)\n                    INNER JOIN temperatures t\n                    ON t.ID = s.ID\n                    WHERE c.Name = ? AND t.Year &gt;= ? AND t.Year &lt;= ? AND t.Month = ?   \n                    ORDER BY NAME                            \n                    \"\"\", conn, params=(country, year_begin, year_end, month))\n        \n        # returns dataframe in order of columns mentioned before\n    return df[[\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\", \"Year\", \"Month\", \"Temp\"]]    \n\n\n\nThis is what the resulting dataframe looks like:\n\ndf = query_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\n3. Write a Geographic Scatter Function for Yearly Temperature Increases\nNow that we have easy access to certain subsets of our data, let’s visualize our data and see what we can learn from it. To do that, I’ll create a function that displays a geographic scatter plot, where eahc point shows the first coefficient of the linear regression model fitted by the temperatures each station in India from 1980 - 2020.\nTo evaluate the coefficients, we’ll need to implement a function that utilizes LinearRegression from sklearn.linear_model.\n\nfrom sklearn.linear_model import LinearRegression\ndef coef(data_group):\n    \"\"\" fits data to a linear regression model and outputs the first coefficient of the fitted model\n\n    Args:\n        data_group (Pandas dataframe): the data for which to fit to a model\n    \n    Returns:\n        \n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs) : \n    \"\"\"creates a geographic scatter plot where each point displays the first coefficent\n            of the linear regression model fitted for each station's temperature in given year range\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n        min_obs (integer): the minimum required number of years of data for any given station\n        **kwargs (optional): keyword arguments for the geographic scatter plot\n    \n    Returns:\n        an interactive geographic scatterplot\n    \"\"\"\n\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Filter out stations with observations less than min_obs\n    df['ObsCount'] = df.groupby('NAME')['NAME'].transform('count')\n    df = df[df['ObsCount'] &gt;= min_obs]\n\n    # Fitting each station data to a linear regression model\n    #   and put the first coefficient of the fitted models into a Pandas dataframe\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4).reset_index()\n\n    # Adding columns for latitude and longitude for each station in coefs\n    lat_lon = df[['NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates('NAME')\n    coefs = coefs.merge(lat_lon, how='left', left_on='NAME', right_on='NAME')\n    \n    # Creating dictionary to match month to month name (for title of plot)\n    month_dict={1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n            5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\",\n            10: \"October\", 11: \"November\", 12: \"December\"}\n    \n    # Preparing the plot\n    fig = px.scatter_mapbox(coefs,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=0,\n                            title = f\"Estimates of yearly increase in termperature in {month_dict[month]} &lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            **kwargs)\n    \n    # Updating margin and colorbar range\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}, \n                      coloraxis=dict(cmax=0.1, cmin=-0.1), # to make colorbar range from -0.1 to 0.1\n                      coloraxis_colorbar=dict(title='Estimated Yearly&lt;br&gt;Increase (°C)&lt;br&gt;')\n                      )\n    \n    return fig\n\n\n\n\nfrom plotly import express as px\n\ncolor_map = px.colors.sequential.RdBu_r\n\nmonth = 1\ncountry = \"India\"\nyear_begin = 1980\nyear_end = 2020\nfig = temperature_coefficient_plot(\"temps.db\",\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   width = 700)\n                                 \nfig.show()\n\n\n\n\nIt seems that a lot stations that surround the edges of India has a positive coefficient, which indicates that there are trends of increasing temperatures over the past decades. Notice how the stations in the central area of India yields negative coefficients.\nLet’s take a look at a geographic scatter plot for the U.S. during the same time period.\n\n\n\n\n\n\n\n4. Create Two More Interesting Figures\nLet’s see how the average annual temperatures has changed over the years.\n\nfrom climate_database import query_country_climate_database\ndf = query_country_climate_database(\"temps.db\", \"India\",1980, 2020)\ndf.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.25\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.25\n1980\n3\n26.30\n\n\n2\nAGARTALA\n23.883\n91.25\n1980\n4\n29.72\n\n\n3\nAGARTALA\n23.883\n91.25\n1980\n5\n27.28\n\n\n4\nAGARTALA\n23.883\n91.25\n1980\n6\n28.56\n\n\n\n\n\n\n\nWe’ll be using the temperature_coefficient_plot, which I’ve created, to implement the heatmap.\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs) : \n    \"\"\"creates a geographic scatter plot where each point displays the first coefficent\n            of the linear regression model fitted for each station's temperature in given year range\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n        min_obs (integer): the minimum required number of years of data for any given station\n        **kwargs (optional): keyword arguments for the geographic scatter plot\n    \n    Returns:\n        an interactive geographic scatterplot\n    \"\"\"\n\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Filter out stations with observations less than min_obs\n    df['ObsCount'] = df.groupby('NAME')['NAME'].transform('count')\n    df = df[df['ObsCount'] &gt;= min_obs]\n\n    # Fitting each station data to a linear regression model\n    #   and put the first coefficient of the fitted models into a Pandas dataframe\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4).reset_index()\n\n    # Adding columns for latitude and longitude for each station in coefs\n    lat_lon = df[['NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates('NAME')\n    coefs = coefs.merge(lat_lon, how='left', left_on='NAME', right_on='NAME')\n    \n    # Creating dictionary to match month to month name (for title of plot)\n    month_dict={1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n            5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\",\n            10: \"October\", 11: \"November\", 12: \"December\"}\n    \n    # Preparing the plot\n    fig = px.scatter_mapbox(coefs,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=0,\n                            title = f\"Estimates of yearly increase in termperature in {month_dict[month]} &lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            **kwargs)\n    \n    # Updating margin and colorbar range\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}, \n                      coloraxis=dict(cmax=0.1, cmin=-0.1), # to make colorbar range from -0.1 to 0.1\n                      coloraxis_colorbar=dict(title='Estimated Yearly&lt;br&gt;Increase (°C)&lt;br&gt;')\n                      )\n    \n    return fig\n\n\n\nBased on the heatmap below, it appears that, over the years, more and more stations have been a higher annual yearly temperature.\n\nfrom climate_database import yearly_avg_temp_heatmap\n\n# setting color map for heatmap\ncolor_map = px.colors.sequential.deep\n\nfig = yearly_avg_temp_heatmap(\"temps.db\", \"India\", 1980, 2020,\n                              color_continuous_scale = color_map)\n\nfig.show()\n\n\n\n\nWe can dig deeper to learn more about our data. Let’s see how the monthly averages in India has changed from 1980 to 2020, and see if there’s a particular month where the averages have increased.\n\nfrom climate_database import monthly_avg_temp_lineplots\n\nfig = monthly_avg_temp_lineplots(\"temps.db\", \"India\", 1980, 2020,\n                                 height = 1500,\n                                 width = 500)\nfig.show()\n\n\n\n\nWhile none of the plots above appear to have a strongly positive linear trend, we can see that, there’s a small positive linear trend in March and April, and from June to November. This indicates that the effects of climate change may be more apparent in those months in India."
  },
  {
    "objectID": "posts/hw0_constructing-visualization-of-penguin-data/index.html",
    "href": "posts/hw0_constructing-visualization-of-penguin-data/index.html",
    "title": "How to Construct a Visuaization of the Palmer Penguins Data Set",
    "section": "",
    "text": "1. Load the Data Set\nBefore we can create visualizations for the Palmer Penguins Data set, we need to first import the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nHere’s the first five rows of the Palmer Penguins data set:\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n2. Import the Seaborn Package and Create a Visualization\nAfter importing the dataset, we can then import the seaborn package. Then, use seaborn.relplot() from the seaborn package to create a scatter plot that compares the body mass (g) to the flipper length (mm) of each penguin for each sex. Notice that there is a 3rd parameter for Sex where Sex = \".\". This is because there is one entry in the Palmer Penguins data set where the sex of that penguin isn’t specified.\n\nimport seaborn as sns\n\nfgrid = sns.relplot(x = \"Body Mass (g)\", \n                    y = \"Flipper Length (mm)\",\n                    hue = \"Sex\", # to color each point by Sex\n                    data = penguins\n                    )\n\nfgrid.fig.suptitle(\"Body Mass (g) vs. Flipper Length (mm)\") # to add title to plot\nfgrid.fig.subplots_adjust(top=0.9) # to adjust placement of title\n\n\n\n\n\n\n\n\nAnd that’s how you can create a simple scatter plot using Seaborn! You can adjust the arguments of sns.relplot() to create different scatter plots using Palmer Penguins data set."
  },
  {
    "objectID": "posts/hw2_web-scraping/index.html",
    "href": "posts/hw2_web-scraping/index.html",
    "title": "Sample",
    "section": "",
    "text": "In this post, I’m going to make a web scraper using Scrapy to scrape https://www.themoviedb.org, and use the scraped data to discover which movies and TV shows have the same actors as the ones in my favorite movie, The Help."
  },
  {
    "objectID": "posts/hw2_web-scraping/index.html#setting-up",
    "href": "posts/hw2_web-scraping/index.html#setting-up",
    "title": "Sample",
    "section": "1.1 Setting Up",
    "text": "1.1 Setting Up\nTo write a webscraper using Scrapy, first navigate to where you want to save your webscraper files on terminal. Then, type the following code in your terminal window. This will initialize your project by creating a series of files for your web scraper. If this is your first time using Scrapy, you may want to first install scrapy by entering pip install scrapy in your terminal window.\n# install scrapy if it's your first time using it\npip install scrapy\nconda activate PIC16B-24F\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nNow, navigate to settings.py and add the following line. This line is implemented to prevent your scraper from downloading too many files while you’re still testing your scraper. You can remove this line after you’re done testing and implementing your web scraper.\n\nCLOSESPIDER_PAGECOUNT = 20"
  },
  {
    "objectID": "posts/hw2_web-scraping/index.html#creating-a-spider",
    "href": "posts/hw2_web-scraping/index.html#creating-a-spider",
    "title": "Sample",
    "section": "1.2 Creating a Spider",
    "text": "1.2 Creating a Spider\nWe’ll now create a spider to crawl (follow links) the page of my favorite movie and extract data. In scrapy, spiders are classes that defines how a site will be scraped. You can read more about it on its official documentation.\nFirst, create a file inside the spiders directory and name it tmdb_spider.py. Then, add the following lines to tmdb_spider.py.\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'    # names the spider `tmdb_spider`\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nTo run the spider, you can run the following command on your terminal window. Feel free to replace the content in subdir= with the subdirectory of your favorite TV show or movie from the TMDB website!\nscrapy crawl tmdb_spider -o movies.csv -a subdir=50014-the-help\nNote that many websites will set up mechanisms to prevent users from using a Scrapy spider on their website by returning a 403 Error. To prevent this error from happening, we we can implement a fake user-agent in settings.py. Now, navigate to settings.py in your TMDB_scraper folder, add the following line:\n\n# settings.py\n\nUSER_AGENT = 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'"
  },
  {
    "objectID": "posts/hw2_web-scraping/index.html#implementing-parsing-methods",
    "href": "posts/hw2_web-scraping/index.html#implementing-parsing-methods",
    "title": "Sample",
    "section": "1.3 Implementing Parsing Methods",
    "text": "1.3 Implementing Parsing Methods\nWe’ll now define three parsing methods for the TmdbSpider class.\nOur first parsing method, parse(self, response), will navigate from a movie page on themoviedb.org to the Full Cast & Crew page. Instead of returning anything, this function will yield a scrapy.Request, and specifies parse_full_credits() as the callback. This callback function, parse_full_credits, will handle the response from the cast page and is responsible for processing the cast information.\nHere’s what the function looks like:\n\n\n    def parse(self, response):\n        \"\"\"starting from a movie page on themoviedb.org, navigates to the Full Cast & Crew page\n\n        i.e. starting from f\"https://www.themoviedb.org/movie/{subdir}\"\n               go to f\"https://www.themoviedb.org/movie/{subdir}/cast\"\n\n        Args:\n            self (Spider): the instance of the spider calling this method\n            response (scrapy.http.Response): the response object containing the movie page data, \n                                            from which the cast page URL will be derived and followed.\n        \"\"\"\n\n        # to get url for Full Cast & Crew page\n        cast_url = self.start_urls[0] + \"cast\"\n\n        # navigates to Full Cast & Crew page\n        yield scrapy.http.Request(url = cast_url, \n                                    callback = self.parse_full_credits)\n\n\n\nThen, we’ll define our second parsing method, parse_full_credits(self, response). Again, this method will not return anything, but rather yield a scrapy.Response for each actor’s page that is reached, and specifies parse_actor_page as the callback.\nHere’s what the function looks like:\n\n\n    def parse_full_credits(self, response):\n        \"\"\"assuming we start on Full Cast & Crew Page, yields a scrapy.Request for each actor listed on \n            this page\n\n        Args:\n            self (Spider): the instance of the spider calling this method\n            response (scrapy.http.Response): the response object containing the movie page data, \n                                            from which the cast page URL will be derived and followed.\n        \"\"\"\n        # to get a list of all cast names (excluding crew)\n        actors =  response.css(\"ol[class='people credits '] div[class=info] a::attr(href)\").getall()\n\n        # yield a scrapy.Request for each actor\n        for actor in actors:\n            actor_url = \"https://www.themoviedb.org\" + actor\n            yield scrapy.http.Request(url = actor_url,\n                                        callback = self.parse_actor_page)\n\n\n\nLastly, we’ll implement parse_actor_page(self, response). This method will scrape the page of an actor and extract data regarding each movie=TV show that the actor has worked in an “Acting” role.\nHere’s what the function looks like:\n\n\n    def parse_actor_page(self, response):\n        \"\"\"assuming we start on page of an actor, scrapes page to get data regading each movie/TV show that\n            actor has worked in an \"Acting\" role\n        \n        Args:\n            self (Spider): the instance of the spider calling this method\n            response (scrapy.http.Response): the response object containing the movie page data, \n                                            from which the cast page URL will be derived and followed.\n\n        Returns:\n            a dictionary containing two key value pairs of the form\n                    {\"actor\": actor_name, \"movie_or_TV_name: movie_or_TV_name\"}\n        \"\"\"\n\n        # extracting actor name from page\n        actor_name = response.css(\"h2[class=title] a::text\").get()  \n\n        # extracting list of all shows/movies actor has been in\n        movie_or_TV_name = response.css(\"a[class=tooltip] bdi::text\").getall()\n        \n        # returning desired dictionary\n        yield {\"actor\": actor_name, \n                \"movie_or_TV_name\": movie_or_TV_name}\n\n\n\nAfter successfully implementing these methods, you can remove the line CLOSESPIDER_PAGECOUNT = 20 in settings.py and run the following line to export the extracted data into a csv file called results.csv in the TMDB_scraper directory.\nscrapy crawl tmdb_spider -o results.csv -a subdir=50014-the-help"
  },
  {
    "objectID": "posts/hw3_webdev/index.html",
    "href": "posts/hw3_webdev/index.html",
    "title": "Web Development with Dash",
    "section": "",
    "text": "Welcome! In this post, I will go over how to create a webapp using Dash by Plotly. By the end of this blog, we’ll be able to create a website that accepts user input!"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-create-the-database-of-messages",
    "href": "posts/hw3_webdev/index.html#function-to-create-the-database-of-messages",
    "title": "Web Development with Dash",
    "section": "Function to Create the Database of Messages",
    "text": "Function to Create the Database of Messages\nWe’ll first make a function to set up a database to receive messages from users. This is what the function looks like:\n\nimport sqlite3\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"creates database of messages, creates a \n        `messages` table if not already in database,\n         and return the connection of message_db\n\n    Returns:\n        message_db: connection to the message_db database\n    \"\"\"\n    global message_db\n\n    if message_db is not None: # if database is not empty\n        return message_db\n    else:\n        # Connect to the database messages_db.sqlite\n        message_db = sqlite3.connect(\"messages_db.sqlite\", \n                                     check_same_thread=False)\n\n        # SQL command to create a `messages` table in \n        #   the database if it does not exist\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()     # saves changes\n        cursor.close()      # closes cursor\n\n        return message_db\n\n# setting up `message` table\nget_message_db()\n\n&lt;sqlite3.Connection at 0x120d30400&gt;"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-insert-a-users-message-into-database",
    "href": "posts/hw3_webdev/index.html#function-to-insert-a-users-message-into-database",
    "title": "Web Development with Dash",
    "section": "Function to Insert a User’s Message into Database",
    "text": "Function to Insert a User’s Message into Database\nNext, we’ll create a function that inserts a user’s inputted message into the database we just created.\n\ndef insert_message(handle, message):\n    \"\"\"\n    Inserts a new message into the database.\n    Args:\n        handle (str): The user handle.\n        message (str): The content of the message.\n    \"\"\"\n\n    # creating a cursor to our database\n    cur = message_db.cursor()\n    cur.execute(\"\"\"INSERT INTO messages (handle, message)\n                    VALUES (?, ?)\n                   \"\"\",\n                   (handle, message))\n    \n    # committing changes to enure row insertion is saved\n    message_db.commit()\n\n    cursor.close() # closing cursor\n    message_db.close()  # closing connection to database"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#callback-function-to-update-components",
    "href": "posts/hw3_webdev/index.html#callback-function-to-update-components",
    "title": "Web Development with Dash",
    "section": "Callback Function to Update Components",
    "text": "Callback Function to Update Components\nNow, we’ll create a callback function to update the components. Before doing so, we need to define our Dash app. Then we’ll implement the callback function submit() to allow user to submit their message along with their name or handle.\n\nimport dash\nfrom dash import html, dcc, Input, Output, State\n\n# creating Dash app\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    # page title\n    html.H1(\"A SIMPLE MESSAGE BANK\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0F807A'}),\n\n    # form for submission\n    html.H2(\"Submit\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Div(\"Your Message:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-message', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Div(\"Your Name or Handle:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-handle', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Button('Submit', id='submit-button',\n                style={'margin-top': '10px', \n                       'background-color': '#0B5551', \n                       'color': 'white', 'border': 'none'}),\n    html.Div(id='message-output'),\n\n    # to view submitted messages\n    html.H2(\"View\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Button('Update', id='refresh-button', \n                style={'background-color': '#4A708B', \n                       'color': 'white'}),\n    html.Div(id='messages-display')\n])\n\n# define callback function to handle form submission\n@app.callback(\n    Output('message-output', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('user-handle', 'value'), \n     State('user-message', 'value')],\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    if not handle or not message: # if user didn't input both\n        return 'Please enter both a handle and a message.'\n    try: \n        insert_message(handle, message) \n        return 'Thank you for submitting a message!'\n    except Exception as e: # return error message if failed to insert message\n        return f'An error occurred: {str(e)}'\n\ndf = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-fetch-random-messages",
    "href": "posts/hw3_webdev/index.html#function-to-fetch-random-messages",
    "title": "Web Development with Dash",
    "section": "Function to Fetch Random Messages",
    "text": "Function to Fetch Random Messages\nThen, we’ll create a function to fetch random messages in our database.\n\ndef random_messages(n):\n    \"\"\"\n    Fetches a random selection of messages from the database.\n    Args:\n        n (int): Number of random messages to retrieve.\n    Returns:\n        list of tuples: A list of messages with their handles.\n    \"\"\"\n\n    # connect to database\n    db = get_message_db()\n\n    # create a cursor\n    cursor = db.cursor()\n\n    # extracting n random messages\n    query = \"SELECT name_or_handle, message FROM messages ORDER BY RANDOM() LIMIT ?\"\n    cursor.execute(query, (n,))\n    messages = cursor.fetchall()\n\n    # close cursor and connection to database\n    cursor.close()\n    db.close()\n    return messages"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#callback-function-to-display-random-messages",
    "href": "posts/hw3_webdev/index.html#callback-function-to-display-random-messages",
    "title": "Web Development with Dash",
    "section": "Callback Function to Display Random Messages",
    "text": "Callback Function to Display Random Messages\n\n@app.callback(\n    Output('messages-display', 'children'),\n    Input('refresh-button', 'n_clicks'),\n    prevent_initial_call=True\n)\ndef view(n_clicks):\n    try:\n        messages = random_messages(5) # extracting 5 random messages\n        return [\n            # displays each message in one line\n            html.Div([\n                html.P(message, style={'font-size': '16px'}),\n                html.P(f\"- {handle}\", style={'font-size': '16px', 'text-align': 'right'})\n            ], style={'margin-bottom': '20px', 'border-bottom': '1px solid #ccc', 'padding-bottom': '10px'})\n            for handle, message in messages\n        ]\n    except Exception as e:\n        # return error message if failed to fetch messages\n        return html.Div(f\"Failed to fetch messages: {str(e)}\")"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#to-view-final-results",
    "href": "posts/hw3_webdev/index.html#to-view-final-results",
    "title": "Web Development with Dash",
    "section": "To View Final Results",
    "text": "To View Final Results\nWe’ll compile all our functions, update our Dash app to display messages that were submitted, and try using our message bank!\n\nimport sqlite3\nimport dash\nfrom dash import html, dcc, Input, Output, State\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"creates database of messages, creates a `messages` table if not already in database,\n         and return the connection of message_db\n\n    Returns:\n        message_db: connection to the message_db database\n    \"\"\"\n    global message_db\n\n    if message_db is not None: # if database is not empty\n        return message_db\n    else:\n        # Connect to the database messages_db.sqlite\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n        # SQL command to create a `messages` table in the database if it does not exist\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()     # saves changes\n        cursor.close()      # closes cursor\n\n        return message_db\n\n# setting up `message` table\nget_message_db()\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    # page title\n    html.H1(\"A SIMPLE MESSAGE BANK\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0F807A'}),\n\n    # form for submission\n    html.H2(\"Submit\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Div(\"Your Message:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-message', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Div(\"Your Name or Handle:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-handle', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Button('Submit', id='submit-button',\n                style={'margin-top': '10px', \n                       'background-color': '#0B5551', \n                       'color': 'white', 'border': 'none'}),\n    html.Div(id='message-output'),\n\n    # to view submitted messages\n    html.H2(\"View\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Button('Update', id='refresh-button', \n                style={'background-color': '#4A708B', \n                       'color': 'white'}),\n    html.Div(id='messages-display')\n])\n\ndef insert_message(handle, message):\n    with sqlite3.connect(\"messages_db.sqlite\") as conn:\n        cur = conn.cursor()\n        cur.execute(\"\"\"INSERT INTO messages \n                    (handle, message) \n                    VALUES (?, ?)\"\"\", \n                    (handle, message))\n        conn.commit()\n\ndef random_messages(n):\n    with sqlite3.connect(\"messages_db.sqlite\") as conn:\n        cur = conn.cursor()\n        cur.execute(\"\"\"SELECT handle, message FROM \n                    messages ORDER BY RANDOM() \n                    LIMIT ?\"\"\", (n,))\n        messages = cur.fetchall()\n    return messages\n\n@app.callback(\n    Output('message-output', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('user-handle', 'value'), \n     State('user-message', 'value')],\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    if not handle or not message:\n        return 'Please enter both a handle and a message.'\n    try:\n        insert_message(handle, message)\n        return 'Thank you for submitting a message!'\n    except Exception as e:\n        return f'An error occurred: {str(e)}'\n\n@app.callback(\n    Output('messages-display', 'children'),\n    Input('refresh-button', 'n_clicks'),\n    prevent_initial_call=True\n)\ndef view(n_clicks):\n    try:\n        messages = random_messages(5) # extracting 5 random messages\n        return [\n            # displays each message in one line\n            html.Div([\n                html.P(message, style={'font-size': '16px'}),\n                html.P(f\"- {handle}\", \n                       style={'font-size': '16px', \n                              'text-align': 'right'})\n            ], style={'margin-bottom': '20px', \n                      'border-bottom': '1px solid #ccc', \n                      'padding-bottom': '10px'})\n            for handle, message in messages\n        ]\n    except Exception as e:\n        # return error message if failed to fetch messages\n        return html.Div(f\"Failed to fetch messages: {str(e)}\")\n\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html",
    "href": "posts/hw4-heat-diffusion/index.html",
    "title": "Heat Diffusion",
    "section": "",
    "text": "Welcome! In this post, I will show you how to conduct a simulation of two-dimensional heat diffusion in various ways: matrix multiplication, sparse matrix in JAX, direct operation with numpy, and with JAX."
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#the-math-and-science-behind-two-dimensional-heat-diffusion",
    "href": "posts/hw4-heat-diffusion/index.html#the-math-and-science-behind-two-dimensional-heat-diffusion",
    "title": "Heat Diffusion",
    "section": "The Math and Science Behind Two-dimensional Heat Diffusion",
    "text": "The Math and Science Behind Two-dimensional Heat Diffusion\nBy definition, heat diffusion is the “process of determining the spatial distribution of temperature on a conductive surface over time by using the heat equation.” You can read more about heat diffusions here.\nThis is the equation for two-dimensional heat-diffusions:\n\\[\n\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2f}{\\partial y^2}.\n\\]\nFor the purpose of demonstration in this post, we will use the following values for N and ε (epsilon).\n\nN = 101\nepsilon = 0.2\n\nThis will be the initial condition for our heat diffusion: putting 1 unit of heat at the midpoint.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#using-matrix-multiplication",
    "href": "posts/hw4-heat-diffusion/index.html#using-matrix-multiplication",
    "title": "Heat Diffusion",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\nLet’s use matrix-vector multiplication to simulate the heat diffusion in the 2D space. The vector here is created by flattening the current solution \\(u_{i, j}^k\\). Each iteration of the update is given by:\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\nIn other words, we view \\(u_{i, j}^k\\) as the element with index \\(N \\times i + j\\) in a vector of length \\(N^2\\). We’ll put the function above in heat_equation.py.\nFollowing the indexing used in advance_time_matvecmul(A, u, epsilon), the matrix A has size \\(N^2 \\times N^2\\), without all-zero rows or all-zero columns. The corresponding A matrix is given by:\n\nn = N * N\ndiagonals = [-4 * np.ones(n), \n             np.ones(n-1), \n             np.ones(n-1), \n             np.ones(n-N), \n             np.ones(n-N)]\n\ndiagonals[1][(N-1)::N] = 0\ndiagonals[2][(N-1)::N] = 0\nA = (np.diag(diagonals[0]) + np.diag(diagonals[1], 1) \n     + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) \n     + np.diag(diagonals[4], -N))\n\nWe will define a function get_A(N) that takes in the value N as the argument and returns the corresponding matrix A in heat_equation.py. This is what the equation looks like:\n\nimport inspect\nfrom heat_equation import get_A\n\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\" Returns the corresponding matrix A according to N\n\n    Returns:\n        N^2 x N^2 matrix without all-zero rows or all-zero columns\n    \"\"\"\n\n    n = N * N\n    diagonals = [-4 * np.ones(n), \n                 np.ones(n-1), \n                 np.ones(n-1), \n                 np.ones(n-N), \n                 np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\n    return A\n\n\n\nLet’s run the simiulation with get_A() and advance_time_matvecmul() fro 2700 iterations and see how long it takes!\n\nadvance_time_matvecmul(A, u0, epsilon)\nget_A(N)\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\nLet’s run the code above for 2700 iterations and see how long it takes!\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport time\n\n# Parameters\nnum_iterations = 2700\ninterval = 300  # Interval for saving snapshots for visualization\n\n# Initial condition: 1 unit of heat at midpoint\nu = np.zeros((N, N))\nu[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A for finite difference\nA = get_A(N)\n\n# Array to store intermediate solutions for visualization\nsnapshots = []\n\n# Run the simulation\nstart_time = time.time()\nfor i in range(num_iterations):\n    u = advance_time_matvecmul(A, u, epsilon)\n    if (i + 1) % interval == 0:\n        snapshots.append(u.copy())\n\n# Measure the simulation time\nsimulation_time = time.time() - start_time\nprint(f\"Simulation time (without visualization): {simulation_time:.2f} seconds\")\n\n# Visualization: 3x3 grid of heatmaps for snapshots\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor idx, ax in enumerate(axes.flatten()):\n    if idx &lt; len(snapshots):\n        im = ax.imshow(snapshots[idx], cmap='viridis', origin='lower')\n        ax.set_title(f\"Iteration {interval * (idx + 1)}\")\n    ax.axis('on')\n\nSimulation time (without visualization): 85.92 seconds\n\n\n\n\n\n\n\n\n\nLooks like that took over a minute to run!"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#sparse-matrix-in-jax",
    "href": "posts/hw4-heat-diffusion/index.html#sparse-matrix-in-jax",
    "title": "Heat Diffusion",
    "section": "Sparse Matrix in jax",
    "text": "Sparse Matrix in jax\nLet’s try running the same simulation using sparse matrix in JAX. To do that, we’ll define a function get_sparse_A(N) that returns A_sp_matrix, which is the same as the matrix A but in a sparse format. This is what get_sparse_A(N) looks like:\n\nfrom heat_equation import get_sparse_A\n\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"Constructs the finite difference matrix for 2D heat diffusion in sparse format.\"\"\"\n    return sparse.BCOO.fromdense(jnp.array(get_A(N)))\n\n\n\n\nfrom heat_equation import get_sparse_A\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nimport timeit\n\n# Parameters\nnum_iterations = 2700\ninterval = 300  # Interval for saving snapshots for visualization\n\n# Initial condition: 1 unit of heat at midpoint\nu = np.zeros((N, N))\nu[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A for finite difference\nA = get_sparse_A(N)\n\n# Array to store intermediate solutions for visualization\nsnapshots = []\n\n# Run the simulation\nstart_time = time.time()\nfor i in range(num_iterations):\n    u = advance_time_matvecmul(A, u, epsilon)\n    if (i + 1) % interval == 0:\n        snapshots.append(u.copy())\n\n# Measure the simulation time\nsimulation_time = time.time() - start_time\nprint(f\"Simulation time (without visualization): {simulation_time:.2f} seconds\")\n\n# Visualization: 3x3 grid of heatmaps for snapshots\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor idx, ax in enumerate(axes.flatten()):\n    if idx &lt; len(snapshots):\n        im = ax.imshow(snapshots[idx], cmap='viridis', origin='lower')\n        ax.set_title(f\"Iteration {interval * (idx + 1)}\")\n    ax.axis('on')\n\nSimulation time (without visualization): 7.82 seconds\n\n\n\n\n\n\n\n\n\nIt only took 13 seconds to run when using sparse matrix in JAX! That’s an impressive improvement! But can we do better?"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#direction-operation-with-numpy",
    "href": "posts/hw4-heat-diffusion/index.html#direction-operation-with-numpy",
    "title": "Heat Diffusion",
    "section": "Direction Operation with numpy",
    "text": "Direction Operation with numpy\nLet’s simplify the matrix multiplications done in advance_time_matvecmul(A, u, epsilon) by using np.roll() in Numpy. To do this, we’ll write a function advance_time_numpy(u, epsilon) that advances the solution by one timestep in the file heat_equation.py. This is what advance_time_numpy(u, epsilon) looks like:\n\nimport inspect\nfrom heat_equation import advance_time_numpy\n\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the heat diffusion solution by one timestep using numpy operations.\"\"\"\n    \n    # Create a padded version of u with zeros around the border\n    u_pad = np.pad(u, pad_width=1, mode='constant', constant_values=0)\n\n    # Calculate the updated values by rolling the array\n    u_new = (1 - 4 * epsilon) * u_pad[1:-1, 1:-1] + \\\n            epsilon * (np.roll(u_pad, shift=1, axis=0)[1:-1, 1:-1] + \n                       np.roll(u_pad, shift=-1, axis=0)[1:-1, 1:-1] +\n                       np.roll(u_pad, shift=1, axis=1)[1:-1, 1:-1] + \n                       np.roll(u_pad, shift=-1, axis=1)[1:-1, 1:-1]) \n    return u_new\n\n\n\n\nfrom heat_equation import get_sparse_A\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nimport timeit\n\n# Parameters\nnum_iterations = 2700\ninterval = 300  # Interval for saving snapshots for visualization\n\n# Initial condition: 1 unit of heat at midpoint\nu = np.zeros((N, N))\nu[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A for finite difference\nA = get_sparse_A(N)\n\n# Array to store intermediate solutions for visualization\nsnapshots = []\n\n# Run the simulation\nstart_time = time.time()\nfor i in range(num_iterations):\n    u = advance_time_numpy(u, epsilon)\n    if (i + 1) % interval == 0:\n        snapshots.append(u.copy())\n\n# Measure the simulation time\nsimulation_time = time.time() - start_time\nprint(f\"Simulation time (without visualization): {simulation_time:.2f} seconds\")\n\n# Visualization: 3x3 grid of heatmaps for snapshots\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor idx, ax in enumerate(axes.flatten()):\n    if idx &lt; len(snapshots):\n        im = ax.imshow(snapshots[idx], cmap='viridis', origin='lower')\n        ax.set_title(f\"Iteration {interval * (idx + 1)}\")\n    ax.axis('on')\n\nSimulation time (without visualization): 0.47 seconds\n\n\n\n\n\n\n\n\n\nAmazing! That only took half a second to run. Let’s see if we can do even better with JAX!"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#with-jax",
    "href": "posts/hw4-heat-diffusion/index.html#with-jax",
    "title": "Heat Diffusion",
    "section": "With jax",
    "text": "With jax\nLet’s use jax to define a function advance_time_jax(u, epsilon) that does similar just-in-time compilation done in advance_time_numpy(u, epsilon). Here’s what advance_time_jax(u, epsilon) looks like:\n\nimport inspect\nfrom heat_equation import advance_time_jax\n\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit\ndef advance_time_jax(u, epsilon):\n    \"\"\"Advances the heat diffusion solution by one timestep using JAX operations.\"\"\"\n    u_pad = jnp.pad(u, pad_width=1, mode='constant', constant_values=0)\n    \n    u_new = (1 - 4 * epsilon) * u_pad[1:-1, 1:-1] + \\\n            epsilon * (jnp.roll(u_pad, shift=1, axis=0)[1:-1, 1:-1] + \n                       jnp.roll(u_pad, shift=-1, axis=0)[1:-1, 1:-1] +\n                       jnp.roll(u_pad, shift=1, axis=1)[1:-1, 1:-1] +\n                       jnp.roll(u_pad, shift=-1, axis=1)[1:-1, 1:-1]) \n    return u_new\n\n\n\n\nfrom heat_equation import get_sparse_A\nimport jax.numpy as jnp\nfrom jax.experimental import sparse\nimport timeit\n\n# Parameters\nnum_iterations = 2700\ninterval = 300  # Interval for saving snapshots for visualization\n\n# Initial condition: 1 unit of heat at midpoint\nu = np.zeros((N, N))\nu[int(N / 2), int(N / 2)] = 1.0\n\n# Get the matrix A for finite difference\nA = get_sparse_A(N)\n\n# Array to store intermediate solutions for visualization\nsnapshots = []\n\n# Run the simulation\nstart_time = time.time()\nfor i in range(num_iterations):\n    u = advance_time_jax(u, epsilon)\n    if (i + 1) % interval == 0:\n        snapshots.append(u.copy())\n\n# Measure the simulation time\nsimulation_time = time.time() - start_time\nprint(f\"Simulation time (without visualization): {simulation_time:.2f} seconds\")\n\n# Visualization: 3x3 grid of heatmaps for snapshots\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor idx, ax in enumerate(axes.flatten()):\n    if idx &lt; len(snapshots):\n        im = ax.imshow(snapshots[idx], cmap='viridis', origin='lower')\n        ax.set_title(f\"Iteration {interval * (idx + 1)}\")\n    ax.axis('on')\n\nSimulation time (without visualization): 0.20 seconds\n\n\n\n\n\n\n\n\n\nLess than half a second! That’s even better than using direction operation with Numpy!"
  },
  {
    "objectID": "posts/hw4-heat-diffusion/index.html#comparison",
    "href": "posts/hw4-heat-diffusion/index.html#comparison",
    "title": "Heat Diffusion",
    "section": "Comparison",
    "text": "Comparison\nIn conclusion, it’s much faster to use JAX to perform the heat diffusion simulation than using traditional NumPy operations or matrix-vector multiplication. By leveraging JAX’s just-in-time (JIT) compilation and automatic differentiation capabilities, we can optimize the performance significantly, especially for large-scale computations that require iterative updates."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html",
    "href": "posts/hw5-image-classification/index.html",
    "title": "Image Classification with Keras",
    "section": "",
    "text": "In this post, we’ll teach a machine learning algorithm to distinguish between pictures of dogs and pictures of cats."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#load-packages-and-obtain-data",
    "href": "posts/hw5-image-classification/index.html#load-packages-and-obtain-data",
    "title": "Image Classification with Keras",
    "section": "1. Load Packages and Obtain Data",
    "text": "1. Load Packages and Obtain Data\n\n# loading packages\nimport os\nimport keras\nfrom keras import utils \nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV3Large\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nAfter loading the necessary packages, we load a sample data set from Kaggle that contains labeled images of cats and dogs.\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nBy running the code chunk above, we’ve created Datasets for training, validation, and testing. Then, run the following code chunk to resize the images to a fixed size of 150x150.\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nThen, run the following code chunk for rapidly reading data. The batch_size determines how many data points are gathered from the directory at once.\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nWe can get a piece of the data set using the take method; e.g. train_ds.take(1) will receive one batch (32 images with labels) from the training data.\nTo briefly explore our data set, we’ll write a function to create a two-row visualization. The first row will show three random pictures of cats. The second row will show three random pictures of dogs.\n\ndef visualize_cats_and_dogs(train_ds):\n    plt.figure(figsize=(10, 6))\n\n    # unbatch the dataset first\n    train_ds = train_ds.unbatch()\n\n    # shuffle the dataset to randomize the images\n    train_ds = train_ds.shuffle(buffer_size=9305) # set buffer size to size of training samples\n\n    # filter out cats and dogs\n    cat_ds = train_ds.filter(lambda image, label: tf.equal(label, 0)).take(3)\n    dog_ds = train_ds.filter(lambda image, label: tf.equal(label, 1)).take(3)\n\n    # initialize the plot for cats\n    for i, (image, label) in enumerate(cat_ds):\n        ax = plt.subplot(2, 3, i + 1)\n        plt.imshow(image.numpy().astype('uint8'))\n        plt.title('Cat')\n        plt.axis('off')\n\n    # initialize the plot for dogs\n    for j, (image, label) in enumerate(dog_ds):\n        ax = plt.subplot(2, 3, j + 4)  # indexing starts from 4 to move to the second row\n        plt.imshow(image.numpy().astype('uint8'))\n        plt.title('Dog')\n        plt.axis('off')\n\n    plt.show()\n\nvisualize_cats_and_dogs(train_ds)\n\n\n\n\n\n\n\n\nThe following line of code will create an iterator called labels_iterator.\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\nUsing this iterator, we’ll compute the number of images in the training data with label 0 (corresponding to \"cat\") and label 1 (corrresponding to \"dog\"). The baseline machine learning model is the model taht always guesses the most frequent label.\n\n# Collect all labels into a list\nlabels = list(labels_iterator)\n\n# Count occurrences of each label\nnum_cats = labels.count(0)\nnum_dogs = labels.count(1)\n\n# Calculate total number of labels\ntotal_labels = len(labels)\n\n# Discuss the baseline model's potential accuracy\nmost_frequent_label_count = max(num_cats, num_dogs)\nbaseline_accuracy = most_frequent_label_count / total_labels\n\nprint(f\"Number of cats: {num_cats}\")\nprint(f\"Number of dogs: {num_dogs}\")\nprint(f\"Total labels: {total_labels}\")\nprint(f\"Baseline model accuracy: {baseline_accuracy:.2%}\")\n\n# This baseline accuracy will be our benchmark.\n\nNumber of cats: 4637\nNumber of dogs: 4668\nTotal labels: 9305\nBaseline model accuracy: 50.17%\n\n\nIn this case, the baseline model would be 50.17% accurate. We will treat this as the benchmark for improvement. In order for our models to be considered good data science achievements, it should do much better than the baseline!"
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#first-model-keras.sequential",
    "href": "posts/hw5-image-classification/index.html#first-model-keras.sequential",
    "title": "Image Classification with Keras",
    "section": "2. First Model: keras.Sequential",
    "text": "2. First Model: keras.Sequential\nWe’ll first create a keras.Sequential model using two Conv2D layers, two MaxPooling2S layers, one Flatten layer, one Denselayer, and one Dropout layer.\n\n# define the model\nmodel1 = Sequential([\n    Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(2, 2),\n    Conv2D(32, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Use sigmoid for binary classification\n])\n\n# compile the model\nmodel1.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# train the model\nhistory = model1.fit(train_ds, \n                     epochs =20, \n                     validation_data=validation_ds)\n\n# plot training history\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Training History')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 222ms/step - accuracy: 0.5188 - loss: 46.7301 - val_accuracy: 0.5959 - val_loss: 0.6664\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 226ms/step - accuracy: 0.6349 - loss: 0.6478 - val_accuracy: 0.6453 - val_loss: 0.6400\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 221ms/step - accuracy: 0.7118 - loss: 0.5712 - val_accuracy: 0.6531 - val_loss: 0.6607\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 228ms/step - accuracy: 0.7776 - loss: 0.4807 - val_accuracy: 0.6556 - val_loss: 0.6590\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 221ms/step - accuracy: 0.8337 - loss: 0.3855 - val_accuracy: 0.6479 - val_loss: 0.6939\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 225ms/step - accuracy: 0.8735 - loss: 0.3135 - val_accuracy: 0.6556 - val_loss: 0.7973\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 40s 272ms/step - accuracy: 0.8954 - loss: 0.2622 - val_accuracy: 0.6664 - val_loss: 0.8296\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 36s 243ms/step - accuracy: 0.9218 - loss: 0.1982 - val_accuracy: 0.6617 - val_loss: 0.9163\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 222ms/step - accuracy: 0.9344 - loss: 0.1742 - val_accuracy: 0.6604 - val_loss: 0.9611\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 220ms/step - accuracy: 0.9432 - loss: 0.1514 - val_accuracy: 0.6685 - val_loss: 1.0226\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 229ms/step - accuracy: 0.9480 - loss: 0.1370 - val_accuracy: 0.6578 - val_loss: 1.1663\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 224ms/step - accuracy: 0.9533 - loss: 0.1234 - val_accuracy: 0.6698 - val_loss: 1.3386\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 234ms/step - accuracy: 0.9580 - loss: 0.1144 - val_accuracy: 0.6733 - val_loss: 1.3887\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 34s 232ms/step - accuracy: 0.9622 - loss: 0.0997 - val_accuracy: 0.6831 - val_loss: 1.0608\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 35s 236ms/step - accuracy: 0.9676 - loss: 0.0949 - val_accuracy: 0.6810 - val_loss: 1.1369\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 225ms/step - accuracy: 0.9721 - loss: 0.0740 - val_accuracy: 0.6724 - val_loss: 1.2527\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 226ms/step - accuracy: 0.9657 - loss: 0.0854 - val_accuracy: 0.6702 - val_loss: 1.0557\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 225ms/step - accuracy: 0.9699 - loss: 0.0844 - val_accuracy: 0.6715 - val_loss: 1.2079\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 226ms/step - accuracy: 0.9723 - loss: 0.0780 - val_accuracy: 0.6763 - val_loss: 1.2703\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 33s 227ms/step - accuracy: 0.9664 - loss: 0.0920 - val_accuracy: 0.6819 - val_loss: 1.1441\n\n\n\n\n\n\n\n\n\nI tried modifying the first and second Conv2D layer to have 16 and 32 filters, respectively. In additon, I set the Dense layer to have 64 units. However, that resulted in the model’s validation accuracy being %49.48 for the first 10 epoches. Moreover, the loss value ended up stablizing to 0.6933 without any improvements beyond epoch 4. So I stopped the training at epoch 12.\nThen, I modified the Dense layer to have 128 units. This resulted in a much better accuracy as the first epoch resulted in a validation accuracy of 59.59%. The accuracy of model 1 stabilized between 65% to 70% during training. This is roughly a 15% improvement in accuracy compared to the baseline. I do see some overfitting in model1 because the training accuracy quickly outgrows the validdation accuracy. In addition, the training accuracy in the end is close to 96%, which is very high."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#second-model-using-data-augmentation",
    "href": "posts/hw5-image-classification/index.html#second-model-using-data-augmentation",
    "title": "Image Classification with Keras",
    "section": "3. Second Model: Using Data Augmentation",
    "text": "3. Second Model: Using Data Augmentation\nFor our second model, we’ll add some data augmentation layers to my model. Data augmentation refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called invariant features of our input images.\nWe’ll randonly flip an image horizontally and vertically and plot the results!\n\nfor images, labels in train_ds.take(1):\n    image = images[0]  # Take the first image in the batch\n\n# Create the RandomFlip layer\nrandom_flip = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")\n\n# Display the original and flipped images\nplt.figure(figsize=(12, 3))  # for better display\nplt.subplot(1, 5, 1)  # for uniform spacing\nplt.imshow(image.numpy().astype('uint8'))  # Convert tensor to uint8 for display\nplt.title(\"Original\")\nfor i in range(2, 6):  # Iterate to display multiple flipped images\n    # Apply the RandomFlip layer and plot\n    flipped_image = random_flip(image[None, ...], training=True)\n    plt.subplot(1, 5, i)\n    plt.imshow(tf.cast(flipped_image[0], tf.int32))  # Cast to int32 for proper image display\n    plt.title(f\"Flipped {i-1}\")\nplt.show()\n\n\n\n\n\n\n\n\nNext, we’ll try randomly rotating the image by ±10% of 360° (i.e., ±36°) and plot the results!\n\n# create the RandomRotation layer\nrandom_rotation = tf.keras.layers.RandomRotation(0.1)  # rotate within ±10% of 360° (i.e., ±36°)\nimage = tf.cast(image, tf.float32) / 255.0  # scale the image to [0, 1]\n\n# display the original and rotated images\nplt.figure(figsize=(10, 2))\nplt.subplot(1, 4, 1)\nplt.imshow(image)\nplt.title(\"Original\")\nfor i in range(2, 5):\n    # apply the RandomRotation layer and plot\n    rotated_image = random_rotation(image[None, ...], training=True)\n    plt.subplot(1, 4, i)\n    plt.imshow(rotated_image[0])  # directly use the image tensor\n    plt.title(f\"Rotated {i-1}\")\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll apply a keras.layers.RandomFlip() layer and a keras.layers.RandomRotation() layer to a keras.models.Sequential!\n\nmodel2 = Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.1),\n    Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(2, 2),\n    Conv2D(32, (3, 3), activation='relu'),  \n    MaxPooling2D(2, 2),\n    Conv2D(64, (3, 3), activation='relu'), \n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dense(256, activation='relu'), \n    Dropout(0.3), \n    Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\nhistory = model2.fit(train_ds, epochs=20, validation_data=validation_ds)\n\n# Plotting the training and validation accuracy\nplt.figure(figsize=(8, 4))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Training History')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 270ms/step - accuracy: 0.5478 - loss: 21.1476 - val_accuracy: 0.6101 - val_loss: 0.6665\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 298ms/step - accuracy: 0.5982 - loss: 0.6710 - val_accuracy: 0.6174 - val_loss: 0.6523\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 288ms/step - accuracy: 0.6212 - loss: 0.6508 - val_accuracy: 0.6333 - val_loss: 0.6389\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.6419 - loss: 0.6337 - val_accuracy: 0.6586 - val_loss: 0.6139\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.6474 - loss: 0.6240 - val_accuracy: 0.6496 - val_loss: 0.6162\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 305ms/step - accuracy: 0.6703 - loss: 0.6174 - val_accuracy: 0.6952 - val_loss: 0.5850\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 295ms/step - accuracy: 0.6975 - loss: 0.5753 - val_accuracy: 0.6999 - val_loss: 0.5680\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 282ms/step - accuracy: 0.7242 - loss: 0.5528 - val_accuracy: 0.7322 - val_loss: 0.5320\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.7294 - loss: 0.5378 - val_accuracy: 0.7347 - val_loss: 0.5189\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 280ms/step - accuracy: 0.7332 - loss: 0.5270 - val_accuracy: 0.7588 - val_loss: 0.5037\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 296ms/step - accuracy: 0.7522 - loss: 0.5093 - val_accuracy: 0.7635 - val_loss: 0.4988\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 279ms/step - accuracy: 0.7667 - loss: 0.4877 - val_accuracy: 0.7825 - val_loss: 0.4753\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 295ms/step - accuracy: 0.7760 - loss: 0.4671 - val_accuracy: 0.7769 - val_loss: 0.4765\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 299ms/step - accuracy: 0.7799 - loss: 0.4687 - val_accuracy: 0.7743 - val_loss: 0.4791\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 305ms/step - accuracy: 0.7909 - loss: 0.4552 - val_accuracy: 0.7928 - val_loss: 0.4518\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 286ms/step - accuracy: 0.7978 - loss: 0.4415 - val_accuracy: 0.7764 - val_loss: 0.4776\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 285ms/step - accuracy: 0.7890 - loss: 0.4426 - val_accuracy: 0.7971 - val_loss: 0.4654\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 286ms/step - accuracy: 0.7980 - loss: 0.4459 - val_accuracy: 0.7966 - val_loss: 0.4740\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 281ms/step - accuracy: 0.8098 - loss: 0.4202 - val_accuracy: 0.8100 - val_loss: 0.4328\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.8142 - loss: 0.4116 - val_accuracy: 0.8070 - val_loss: 0.4507\n\n\n\n\n\n\n\n\n\nThe accuracy of model 2 stabilized between 75% to 81% during training. This is roughly a 10% improvement in accuracy compared to model1. I don’t see overfitting in model2 because the training accuracy and validation accuracy closely follow each other in the plot above, and the final training accuracy is roughly 80%, which is reasonable."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#third-model-using-data-preprocessing",
    "href": "posts/hw5-image-classification/index.html#third-model-using-data-preprocessing",
    "title": "Image Classification with Keras",
    "section": "4. Third Model: Using Data Preprocessing",
    "text": "4. Third Model: Using Data Preprocessing\nIt can sometimes be helpul to make simple transformations to the input data so that we can improve our model’s validation accuracy.\nFor example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.\nThe following code chunk will create a preprocessing layer called preprocessor, which we can add to our model pipeline.\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = i, outputs = x)\n\n\nmodel3 = Sequential([\n    preprocessor, \n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.1),\n    Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(2, 2),\n    Conv2D(32, (3, 3), activation='relu'),  \n    MaxPooling2D(2, 2),\n    Conv2D(64, (3, 3), activation='relu'), \n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dense(256, activation='relu'), \n    Dropout(0.3), \n    Dense(1, activation='sigmoid')\n])\n\nmodel3.compile(optimizer='adam',\n               loss='binary_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model3.fit(train_ds, epochs=20, validation_data=validation_ds)\nplt.figure(figsize=(8, 4))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Training History')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 296ms/step - accuracy: 0.5760 - loss: 0.7294 - val_accuracy: 0.7016 - val_loss: 0.5778\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 299ms/step - accuracy: 0.6809 - loss: 0.5910 - val_accuracy: 0.7515 - val_loss: 0.5141\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 326ms/step - accuracy: 0.7254 - loss: 0.5434 - val_accuracy: 0.7756 - val_loss: 0.4755\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 53s 361ms/step - accuracy: 0.7497 - loss: 0.5120 - val_accuracy: 0.7885 - val_loss: 0.4517\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 47s 320ms/step - accuracy: 0.7773 - loss: 0.4793 - val_accuracy: 0.8014 - val_loss: 0.4302\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 286ms/step - accuracy: 0.7807 - loss: 0.4666 - val_accuracy: 0.8057 - val_loss: 0.4278\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 293ms/step - accuracy: 0.7970 - loss: 0.4407 - val_accuracy: 0.8022 - val_loss: 0.4405\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 282ms/step - accuracy: 0.8004 - loss: 0.4331 - val_accuracy: 0.8169 - val_loss: 0.4101\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 40s 277ms/step - accuracy: 0.8173 - loss: 0.4116 - val_accuracy: 0.8259 - val_loss: 0.3985\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 300ms/step - accuracy: 0.8266 - loss: 0.3931 - val_accuracy: 0.8285 - val_loss: 0.3932\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 46s 312ms/step - accuracy: 0.8282 - loss: 0.3845 - val_accuracy: 0.8237 - val_loss: 0.3980\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 332ms/step - accuracy: 0.8344 - loss: 0.3786 - val_accuracy: 0.8379 - val_loss: 0.3832\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 327ms/step - accuracy: 0.8417 - loss: 0.3546 - val_accuracy: 0.8353 - val_loss: 0.3912\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 308ms/step - accuracy: 0.8555 - loss: 0.3381 - val_accuracy: 0.8366 - val_loss: 0.3827\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.8476 - loss: 0.3444 - val_accuracy: 0.8362 - val_loss: 0.3617\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.8574 - loss: 0.3262 - val_accuracy: 0.8375 - val_loss: 0.3729\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 301ms/step - accuracy: 0.8569 - loss: 0.3256 - val_accuracy: 0.8405 - val_loss: 0.3806\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 280ms/step - accuracy: 0.8699 - loss: 0.3071 - val_accuracy: 0.8418 - val_loss: 0.3676\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 284ms/step - accuracy: 0.8809 - loss: 0.2964 - val_accuracy: 0.8375 - val_loss: 0.3875\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 45s 310ms/step - accuracy: 0.8791 - loss: 0.2853 - val_accuracy: 0.8401 - val_loss: 0.3871\n\n\n\n\n\n\n\n\n\nThe accuracy of model 3 stabilized between 80% to 85% during training. This is roughly a 15% improvement in accuracy compared to model1. I don’t see overfitting in model3, because even though the training accuracy is higher than the valifation accuracy, the gap between the two values in the plot appear to be small. In addition, the final training accuracy is roughly 87%, which is reasonable."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#fourth-model-using-transfer-learning",
    "href": "posts/hw5-image-classification/index.html#fourth-model-using-transfer-learning",
    "title": "Image Classification with Keras",
    "section": "5. Fourth Model: Using Transfer Learning",
    "text": "5. Fourth Model: Using Transfer Learning\nLet’s use model3 as a base model, incorporate into a full model, and then train that model as our fourth model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n\nmodel4 = keras.Sequential([\n    # data augmentation layers from model3\n    keras.layers.RandomFlip(\"horizontal\"),\n    keras.layers.RandomRotation(0.1),\n    \n    base_model,\n    keras.layers.GlobalMaxPooling2D(),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(2, activation='softmax')\n])\n\nmodel4.compile(optimizer='adam',\n               loss='sparse_categorical_crossentropy',\n               metrics=['accuracy'])\n\n# Train the model\nhistory = model4.fit(train_ds, epochs=20, validation_data=validation_ds)\n\n# Plot training history\nplt.figure(figsize=(8, 4))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Training History')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 50s 319ms/step - accuracy: 0.7765 - loss: 2.4293 - val_accuracy: 0.9622 - val_loss: 0.2711\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 46s 316ms/step - accuracy: 0.9331 - loss: 0.5682 - val_accuracy: 0.9678 - val_loss: 0.2334\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 280ms/step - accuracy: 0.9451 - loss: 0.4454 - val_accuracy: 0.9690 - val_loss: 0.2053\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 304ms/step - accuracy: 0.9533 - loss: 0.3284 - val_accuracy: 0.9673 - val_loss: 0.2196\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 292ms/step - accuracy: 0.9557 - loss: 0.3387 - val_accuracy: 0.9656 - val_loss: 0.2315\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 286ms/step - accuracy: 0.9503 - loss: 0.3845 - val_accuracy: 0.9690 - val_loss: 0.1929\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 291ms/step - accuracy: 0.9541 - loss: 0.2786 - val_accuracy: 0.9690 - val_loss: 0.1901\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 43s 293ms/step - accuracy: 0.9492 - loss: 0.2987 - val_accuracy: 0.9617 - val_loss: 0.2298\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 285ms/step - accuracy: 0.9493 - loss: 0.2922 - val_accuracy: 0.9686 - val_loss: 0.1842\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 41s 283ms/step - accuracy: 0.9570 - loss: 0.2350 - val_accuracy: 0.9755 - val_loss: 0.1329\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 302ms/step - accuracy: 0.9593 - loss: 0.2209 - val_accuracy: 0.9716 - val_loss: 0.1661\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 44s 300ms/step - accuracy: 0.9576 - loss: 0.2291 - val_accuracy: 0.9729 - val_loss: 0.1467\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 49s 338ms/step - accuracy: 0.9562 - loss: 0.2177 - val_accuracy: 0.9733 - val_loss: 0.1378\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 46s 312ms/step - accuracy: 0.9487 - loss: 0.2762 - val_accuracy: 0.9742 - val_loss: 0.1366\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 332ms/step - accuracy: 0.9516 - loss: 0.2467 - val_accuracy: 0.9669 - val_loss: 0.1626\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 47s 321ms/step - accuracy: 0.9551 - loss: 0.2051 - val_accuracy: 0.9751 - val_loss: 0.1430\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 327ms/step - accuracy: 0.9568 - loss: 0.2131 - val_accuracy: 0.9686 - val_loss: 0.1469\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 290ms/step - accuracy: 0.9492 - loss: 0.2457 - val_accuracy: 0.9742 - val_loss: 0.1621\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 48s 331ms/step - accuracy: 0.9578 - loss: 0.2283 - val_accuracy: 0.9742 - val_loss: 0.1293\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 46s 317ms/step - accuracy: 0.9613 - loss: 0.1635 - val_accuracy: 0.9712 - val_loss: 0.1554\n\n\n\n\n\n\n\n\n\nLet’s check model4.summary() to see how many parameters we had to train in this model.\n\nmodel4.summary()\n\nModel: \"sequential_119\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ random_flip_88 (RandomFlip)     │ (None, 150, 150, 3)    │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ random_rotation_91              │ (None, 150, 150, 3)    │             0 │\n│ (RandomRotation)                │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ MobileNetV3Large (Functional)   │ (None, 5, 5, 960)      │     2,996,352 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_max_pooling2d_16         │ (None, 960)            │             0 │\n│ (GlobalMaxPooling2D)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_107 (Dropout)           │ (None, 960)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_178 (Dense)               │ (None, 2)              │         1,922 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 3,002,120 (11.45 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\n Optimizer params: 3,846 (15.03 KB)\n\n\n\nAccording to the summary table above, we trained a total of 1992 parameters, which is a lot!\nThe accuracy of model 4 stabilized between 96% to 98% during training. This is roughly a 30% improvement in accuracy compared to model1. There appears to be minor overfitting since there appears to be a noticeable difference between the training accuracy and the validation accuracy."
  },
  {
    "objectID": "posts/hw5-image-classification/index.html#score-on-test-data",
    "href": "posts/hw5-image-classification/index.html#score-on-test-data",
    "title": "Image Classification with Keras",
    "section": "6. Score on Test Data",
    "text": "6. Score on Test Data\nIt looks like model4 performed the best out of the four models I’ve demonstrated thus far. Let’s score on the test data using model4.\n\ntest_loss, test_accuracy = model4.evaluate(test_ds)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 10s 255ms/step - accuracy: 0.9685 - loss: 0.1355\nTest Loss: 0.15898022055625916\nTest Accuracy: 0.9673258662223816\n\n\nIt seems that model4 scored a 96.73% accuracy when scored on the test data!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Detecting Fake vs. Real News",
    "section": "",
    "text": "In this post, I will show you how to develop and assess a fake news classifier using Keras."
  },
  {
    "objectID": "posts/index.html#create-models",
    "href": "posts/index.html#create-models",
    "title": "Detecting Fake vs. Real News",
    "section": "3. Create Models",
    "text": "3. Create Models\nWe will create three Keras model to offer a perspective the following question:\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\nFor the first model, we will use only the article title as input.\n\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\ntrain_data = train.map(lambda x, y: x[\"title\"])\ntitle_vectorize_layer.adapt(train_data)\n\n# input layer\ntitle_input = keras.Input(shape=(1,), dtype=\"string\", name=\"title\")\n\n# text processing layers\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(title_features)\n\n\n# model compilation (need to do this before training)\nmodel1 = keras.Model(inputs=title_input, outputs=output)\nmodel1.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel1.summary()\n\nModel: \"functional_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_8 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_12          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_9 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nfrom keras import utils\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we train our model.\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.5263 - loss: 0.6890 - val_accuracy: 0.8038 - val_loss: 0.6404\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 27ms/step - accuracy: 0.8175 - loss: 0.5799 - val_accuracy: 0.9451 - val_loss: 0.3571\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9034 - loss: 0.3341 - val_accuracy: 0.9218 - val_loss: 0.2310\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9200 - loss: 0.2399 - val_accuracy: 0.9253 - val_loss: 0.1896\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9359 - loss: 0.1982 - val_accuracy: 0.9187 - val_loss: 0.1785\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 33ms/step - accuracy: 0.9346 - loss: 0.1861 - val_accuracy: 0.9178 - val_loss: 0.1721\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 23ms/step - accuracy: 0.9419 - loss: 0.1732 - val_accuracy: 0.9304 - val_loss: 0.1544\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9456 - loss: 0.1605 - val_accuracy: 0.9420 - val_loss: 0.1401\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9461 - loss: 0.1510 - val_accuracy: 0.9318 - val_loss: 0.1473\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 31ms/step - accuracy: 0.9489 - loss: 0.1432 - val_accuracy: 0.9484 - val_loss: 0.1268\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9529 - loss: 0.1387 - val_accuracy: 0.9518 - val_loss: 0.1197\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9566 - loss: 0.1315 - val_accuracy: 0.9736 - val_loss: 0.1140\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9585 - loss: 0.1236 - val_accuracy: 0.9498 - val_loss: 0.1200\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1222 - val_accuracy: 0.9529 - val_loss: 0.1128\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.9593 - loss: 0.1132 - val_accuracy: 0.9458 - val_loss: 0.1232\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9578 - loss: 0.1207 - val_accuracy: 0.9504 - val_loss: 0.1169\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9614 - loss: 0.1091 - val_accuracy: 0.9544 - val_loss: 0.1068\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9612 - loss: 0.1090 - val_accuracy: 0.9549 - val_loss: 0.1049\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9593 - loss: 0.1082 - val_accuracy: 0.9564 - val_loss: 0.1032\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9630 - loss: 0.1005 - val_accuracy: 0.9551 - val_loss: 0.1044\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9616 - loss: 0.1012 - val_accuracy: 0.9540 - val_loss: 0.1082\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9661 - loss: 0.0966 - val_accuracy: 0.9549 - val_loss: 0.1027\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9638 - loss: 0.0972 - val_accuracy: 0.9544 - val_loss: 0.1038\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9652 - loss: 0.0931 - val_accuracy: 0.9780 - val_loss: 0.0962\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9687 - loss: 0.0883 - val_accuracy: 0.9776 - val_loss: 0.0956\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9664 - loss: 0.0908 - val_accuracy: 0.9553 - val_loss: 0.1004\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9662 - loss: 0.0882 - val_accuracy: 0.9769 - val_loss: 0.0926\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9686 - loss: 0.0857 - val_accuracy: 0.9776 - val_loss: 0.0880\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.9705 - loss: 0.0806 - val_accuracy: 0.9782 - val_loss: 0.0901\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9715 - loss: 0.0789 - val_accuracy: 0.9789 - val_loss: 0.0889\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9700 - loss: 0.0803 - val_accuracy: 0.9778 - val_loss: 0.0918\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9745 - loss: 0.0742 - val_accuracy: 0.9784 - val_loss: 0.0868\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9715 - loss: 0.0752 - val_accuracy: 0.9756 - val_loss: 0.0914\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9698 - loss: 0.0771 - val_accuracy: 0.9780 - val_loss: 0.0927\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9681 - loss: 0.0814 - val_accuracy: 0.9567 - val_loss: 0.0978\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.0743 - val_accuracy: 0.9784 - val_loss: 0.0915\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0834 - val_accuracy: 0.9780 - val_loss: 0.0843\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9756 - loss: 0.0672 - val_accuracy: 0.9769 - val_loss: 0.0861\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9720 - loss: 0.0735 - val_accuracy: 0.9767 - val_loss: 0.0893\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9740 - loss: 0.0682 - val_accuracy: 0.9778 - val_loss: 0.0846\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9758 - loss: 0.0667 - val_accuracy: 0.9767 - val_loss: 0.0945\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9747 - loss: 0.0656 - val_accuracy: 0.9780 - val_loss: 0.0913\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model1’s accuracy ranges between 96% and 97.5%.\nFor the second model, we will use only the article text as input. Once again, we’ll compile and then train the model.\n\ntrain_texts = train.map(lambda x, y: x['text'])\ntext_vectorize_layer.adapt(train_texts)\n\n# input Layer\ntext_input = keras.Input(shape=(1,), dtype=\"string\", name=\"text\")\n\n# text processing layers\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(text_features)\n\n# Model Compilation\nmodel2 = keras.Model(inputs=text_input, outputs=output)\nmodel2.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\nmodel2.summary()\n\nModel: \"functional_8\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_10 (Dropout)                 │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_13          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_11 (Dropout)                 │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 21ms/step - accuracy: 0.5335 - loss: 0.6873 - val_accuracy: 0.8627 - val_loss: 0.6162\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 42ms/step - accuracy: 0.8523 - loss: 0.5490 - val_accuracy: 0.9489 - val_loss: 0.3287\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9127 - loss: 0.3132 - val_accuracy: 0.9229 - val_loss: 0.2216\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9291 - loss: 0.2258 - val_accuracy: 0.9189 - val_loss: 0.1876\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9337 - loss: 0.1943 - val_accuracy: 0.9260 - val_loss: 0.1683\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9443 - loss: 0.1668 - val_accuracy: 0.9358 - val_loss: 0.1487\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9461 - loss: 0.1542 - val_accuracy: 0.9356 - val_loss: 0.1424\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9473 - loss: 0.1461 - val_accuracy: 0.9447 - val_loss: 0.1310\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9509 - loss: 0.1354 - val_accuracy: 0.9478 - val_loss: 0.1252\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9534 - loss: 0.1321 - val_accuracy: 0.9371 - val_loss: 0.1382\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9532 - loss: 0.1277 - val_accuracy: 0.9498 - val_loss: 0.1184\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1149 - val_accuracy: 0.9513 - val_loss: 0.1149\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9591 - loss: 0.1120 - val_accuracy: 0.9609 - val_loss: 0.1018\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9618 - loss: 0.1044 - val_accuracy: 0.9542 - val_loss: 0.1078\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9639 - loss: 0.1021 - val_accuracy: 0.9529 - val_loss: 0.1085\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9631 - loss: 0.0975 - val_accuracy: 0.9609 - val_loss: 0.0968\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9620 - loss: 0.0958 - val_accuracy: 0.9458 - val_loss: 0.1157\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1071 - val_accuracy: 0.9542 - val_loss: 0.1060\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9673 - loss: 0.0880 - val_accuracy: 0.9533 - val_loss: 0.1069\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9646 - loss: 0.0891 - val_accuracy: 0.9598 - val_loss: 0.0926\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9667 - loss: 0.0844 - val_accuracy: 0.9580 - val_loss: 0.0994\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9658 - loss: 0.0888 - val_accuracy: 0.9602 - val_loss: 0.0916\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9685 - loss: 0.0788 - val_accuracy: 0.9600 - val_loss: 0.0923\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9702 - loss: 0.0755 - val_accuracy: 0.9633 - val_loss: 0.0896\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0788 - val_accuracy: 0.9531 - val_loss: 0.1074\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9712 - loss: 0.0748 - val_accuracy: 0.9593 - val_loss: 0.0921\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9726 - loss: 0.0703 - val_accuracy: 0.9607 - val_loss: 0.0903\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9743 - loss: 0.0654 - val_accuracy: 0.9596 - val_loss: 0.0916\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9755 - loss: 0.0656 - val_accuracy: 0.9482 - val_loss: 0.1184\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model2’s accuracy ranges between 96.5% and 97.5%, which is a slight improvement compared to model1.\nFor the third model, we will use both the article title and the article text as input.\n\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Dropout, GlobalAveragePooling1D, Dense, Input, concatenate\n\ntrain_data = train.map(lambda x, y: (x[\"title\"], x[\"text\"]))\ntrain_titles, train_texts = zip(*train_data)\n\nshared_embedding = Embedding(size_vocabulary, 3, name = \"embedding\")\n\ntitle_input = Input(shape=(1,), dtype=tf.string, name=\"title\")\ntitle_vectorized = title_vectorize_layer(title_input)\ntitle_embedded = shared_embedding(title_vectorized)\ntitle_processed = GlobalAveragePooling1D()(title_embedded)\n\ntext_input = Input(shape=(1,), dtype=tf.string, name=\"text\")\ntext_vectorized = text_vectorize_layer(text_input)\ntext_embedded = shared_embedding(text_vectorized)\ntext_processed = GlobalAveragePooling1D()(text_embedded)\n\ncombined_features = concatenate([title_processed, text_processed])\noutput = Dense(1, activation='sigmoid')(combined_features)\n\nmodel3 = keras.Model(inputs=[title_input, text_input], outputs=output)\nmodel3.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel3.summary()\n\nModel: \"functional_11\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 3)         │          6,000 │ text_vectorization[7]… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[1][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_4             │ (None, 6)              │              0 │ global_average_poolin… │\n│ (Concatenate)             │                        │                │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_16 (Dense)          │ (None, 1)              │              7 │ concatenate_4[0][0]    │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 6,007 (23.46 KB)\n\n\n\n Trainable params: 6,007 (23.46 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.5798 - loss: 0.6851 - val_accuracy: 0.6607 - val_loss: 0.6560\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.6959 - loss: 0.6448 - val_accuracy: 0.7358 - val_loss: 0.6053\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.7639 - loss: 0.5922 - val_accuracy: 0.8273 - val_loss: 0.5465\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.8506 - loss: 0.5326 - val_accuracy: 0.9038 - val_loss: 0.4869\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9067 - loss: 0.4742 - val_accuracy: 0.9291 - val_loss: 0.4338\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9252 - loss: 0.4234 - val_accuracy: 0.9378 - val_loss: 0.3895\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9342 - loss: 0.3814 - val_accuracy: 0.9424 - val_loss: 0.3534\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 36ms/step - accuracy: 0.9386 - loss: 0.3473 - val_accuracy: 0.9447 - val_loss: 0.3241\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9416 - loss: 0.3195 - val_accuracy: 0.9464 - val_loss: 0.3000\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9448 - loss: 0.2966 - val_accuracy: 0.9473 - val_loss: 0.2800\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 30ms/step - accuracy: 0.9460 - loss: 0.2776 - val_accuracy: 0.9496 - val_loss: 0.2631\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9478 - loss: 0.2614 - val_accuracy: 0.9518 - val_loss: 0.2488\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9500 - loss: 0.2476 - val_accuracy: 0.9527 - val_loss: 0.2364\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9514 - loss: 0.2357 - val_accuracy: 0.9556 - val_loss: 0.2257\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9527 - loss: 0.2252 - val_accuracy: 0.9558 - val_loss: 0.2162\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9541 - loss: 0.2159 - val_accuracy: 0.9556 - val_loss: 0.2079\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9552 - loss: 0.2077 - val_accuracy: 0.9564 - val_loss: 0.2004\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9564 - loss: 0.2002 - val_accuracy: 0.9576 - val_loss: 0.1937\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9573 - loss: 0.1935 - val_accuracy: 0.9589 - val_loss: 0.1876\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9583 - loss: 0.1874 - val_accuracy: 0.9602 - val_loss: 0.1821\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9598 - loss: 0.1818 - val_accuracy: 0.9618 - val_loss: 0.1771\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1767 - val_accuracy: 0.9627 - val_loss: 0.1725\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9615 - loss: 0.1719 - val_accuracy: 0.9636 - val_loss: 0.1683\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 22ms/step - accuracy: 0.9618 - loss: 0.1675 - val_accuracy: 0.9638 - val_loss: 0.1644\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9626 - loss: 0.1634 - val_accuracy: 0.9642 - val_loss: 0.1607\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9631 - loss: 0.1596 - val_accuracy: 0.9649 - val_loss: 0.1574\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9638 - loss: 0.1560 - val_accuracy: 0.9651 - val_loss: 0.1542\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9641 - loss: 0.1526 - val_accuracy: 0.9667 - val_loss: 0.1513\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9655 - loss: 0.1495 - val_accuracy: 0.9669 - val_loss: 0.1485\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9658 - loss: 0.1464 - val_accuracy: 0.9680 - val_loss: 0.1460\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9665 - loss: 0.1436 - val_accuracy: 0.9682 - val_loss: 0.1435\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9668 - loss: 0.1409 - val_accuracy: 0.9684 - val_loss: 0.1412\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9677 - loss: 0.1383 - val_accuracy: 0.9693 - val_loss: 0.1391\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9681 - loss: 0.1359 - val_accuracy: 0.9696 - val_loss: 0.1370\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9682 - loss: 0.1335 - val_accuracy: 0.9702 - val_loss: 0.1351\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9689 - loss: 0.1313 - val_accuracy: 0.9700 - val_loss: 0.1332\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9690 - loss: 0.1291 - val_accuracy: 0.9702 - val_loss: 0.1315\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9694 - loss: 0.1271 - val_accuracy: 0.9711 - val_loss: 0.1298\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 38ms/step - accuracy: 0.9695 - loss: 0.1251 - val_accuracy: 0.9713 - val_loss: 0.1282\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 27ms/step - accuracy: 0.9697 - loss: 0.1232 - val_accuracy: 0.9718 - val_loss: 0.1267\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9700 - loss: 0.1213 - val_accuracy: 0.9727 - val_loss: 0.1252\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9701 - loss: 0.1196 - val_accuracy: 0.9729 - val_loss: 0.1238\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9707 - loss: 0.1178 - val_accuracy: 0.9727 - val_loss: 0.1224\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.1162 - val_accuracy: 0.9729 - val_loss: 0.1211\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9718 - loss: 0.1146 - val_accuracy: 0.9729 - val_loss: 0.1199\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9721 - loss: 0.1130 - val_accuracy: 0.9733 - val_loss: 0.1187\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9723 - loss: 0.1115 - val_accuracy: 0.9731 - val_loss: 0.1176\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 35ms/step - accuracy: 0.9725 - loss: 0.1100 - val_accuracy: 0.9736 - val_loss: 0.1164\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 24ms/step - accuracy: 0.9729 - loss: 0.1086 - val_accuracy: 0.9736 - val_loss: 0.1154\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9731 - loss: 0.1072 - val_accuracy: 0.9738 - val_loss: 0.1143\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nCompared to the previous two models, model3 appears to perform the best, because its accuracy ranges between 97% and 97.31% towards the last 10 epochs. Though it’s not as high as the other two models, model3 is able to consistently score at least 97% validation accuracy. Whereas the other two models often fluctuate between 96% and 97%."
  },
  {
    "objectID": "posts/index.html#model-evaluation",
    "href": "posts/index.html#model-evaluation",
    "title": "Detecting Fake vs. Real News",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\nWe will now use model3 to test my model performance on unseen data.\n\n# load test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(train_url)\ntest_data.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntest = make_dataset(test_data)\nloss, accuracy = model3.evaluate(test)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9737 - loss: 0.1090\nTest Loss: 0.10588612407445908\nTest Accuracy: 0.9754109382629395\n\n\nIt appears that model3 managed to get a test accuracy of 97.5%, which is great!"
  },
  {
    "objectID": "posts/index.html#embedding-visualization",
    "href": "posts/index.html#embedding-visualization",
    "title": "Detecting Fake vs. Real News",
    "section": "5. Embedding Visualization",
    "text": "5. Embedding Visualization\nLet’s visualize and comment on the embedding that model3 learned.\n\n# to display plotly plots on quarto\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nweights = model3.get_layer('embedding').get_weights()[0]\n\nvocab = title_vectorize_layer.get_vocabulary()\n\npca = PCA(n_components=2)\nreduced_weights = pca.fit_transform(weights)\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': reduced_weights[:, 0],\n    'x1': reduced_weights[:, 1]\n})\n\nfig = px.scatter(embedding_df,\n                 x=\"x0\",\n                 y=\"x1\",\n                 hover_name=\"word\",  # shows the word on hover\n                 title=\"2D PCA of Word Embeddings\",\n                 width=800,\n                 height=600)\nfig.update_traces(marker=dict(size=5,\n                              line=dict(width=1,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.show()\n\n\n\n\nIn the graph above, the embedding of the words “Wednesday”, “Thursday”, “Monday”, “Tueday”, and “Friday” appear to be clustered together but not inside the central cluster. Moreover, their x1 values appear to be close to 0. Whereas their x0 values appear to range between -16 and -19. Since they are clustered outside of the central cluster, this means that these words likely do not help one differentiate between fake and real news.\nMoreover, the fact that they are clustered together also makes sense, because all of them are days of the week."
  },
  {
    "objectID": "posts/hw6/index.html",
    "href": "posts/hw6/index.html",
    "title": "Sample",
    "section": "",
    "text": "In this post, I will show you how to develop and assess a fake news classifier using Keras."
  },
  {
    "objectID": "posts/hw6/index.html#create-models",
    "href": "posts/hw6/index.html#create-models",
    "title": "Sample",
    "section": "3. Create Models",
    "text": "3. Create Models\nWe will create three Keras model to offer a perspective the following question:\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\nFor the first model, we will use only the article title as input.\n\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\ntrain_data = train.map(lambda x, y: x[\"title\"])\ntitle_vectorize_layer.adapt(train_data)\n\n# input layer\ntitle_input = keras.Input(shape=(1,), dtype=\"string\", name=\"title\")\n\n# text processing layers\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(title_features)\n\n\n# model compilation (need to do this before training)\nmodel1 = keras.Model(inputs=title_input, outputs=output)\nmodel1.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel1.summary()\n\nModel: \"functional_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_8 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_12          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_9 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nfrom keras import utils\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we train our model.\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.5263 - loss: 0.6890 - val_accuracy: 0.8038 - val_loss: 0.6404\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 27ms/step - accuracy: 0.8175 - loss: 0.5799 - val_accuracy: 0.9451 - val_loss: 0.3571\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9034 - loss: 0.3341 - val_accuracy: 0.9218 - val_loss: 0.2310\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9200 - loss: 0.2399 - val_accuracy: 0.9253 - val_loss: 0.1896\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9359 - loss: 0.1982 - val_accuracy: 0.9187 - val_loss: 0.1785\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 33ms/step - accuracy: 0.9346 - loss: 0.1861 - val_accuracy: 0.9178 - val_loss: 0.1721\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 23ms/step - accuracy: 0.9419 - loss: 0.1732 - val_accuracy: 0.9304 - val_loss: 0.1544\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9456 - loss: 0.1605 - val_accuracy: 0.9420 - val_loss: 0.1401\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9461 - loss: 0.1510 - val_accuracy: 0.9318 - val_loss: 0.1473\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 31ms/step - accuracy: 0.9489 - loss: 0.1432 - val_accuracy: 0.9484 - val_loss: 0.1268\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9529 - loss: 0.1387 - val_accuracy: 0.9518 - val_loss: 0.1197\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9566 - loss: 0.1315 - val_accuracy: 0.9736 - val_loss: 0.1140\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9585 - loss: 0.1236 - val_accuracy: 0.9498 - val_loss: 0.1200\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1222 - val_accuracy: 0.9529 - val_loss: 0.1128\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.9593 - loss: 0.1132 - val_accuracy: 0.9458 - val_loss: 0.1232\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9578 - loss: 0.1207 - val_accuracy: 0.9504 - val_loss: 0.1169\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9614 - loss: 0.1091 - val_accuracy: 0.9544 - val_loss: 0.1068\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9612 - loss: 0.1090 - val_accuracy: 0.9549 - val_loss: 0.1049\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9593 - loss: 0.1082 - val_accuracy: 0.9564 - val_loss: 0.1032\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9630 - loss: 0.1005 - val_accuracy: 0.9551 - val_loss: 0.1044\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9616 - loss: 0.1012 - val_accuracy: 0.9540 - val_loss: 0.1082\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9661 - loss: 0.0966 - val_accuracy: 0.9549 - val_loss: 0.1027\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9638 - loss: 0.0972 - val_accuracy: 0.9544 - val_loss: 0.1038\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9652 - loss: 0.0931 - val_accuracy: 0.9780 - val_loss: 0.0962\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9687 - loss: 0.0883 - val_accuracy: 0.9776 - val_loss: 0.0956\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9664 - loss: 0.0908 - val_accuracy: 0.9553 - val_loss: 0.1004\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9662 - loss: 0.0882 - val_accuracy: 0.9769 - val_loss: 0.0926\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9686 - loss: 0.0857 - val_accuracy: 0.9776 - val_loss: 0.0880\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.9705 - loss: 0.0806 - val_accuracy: 0.9782 - val_loss: 0.0901\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9715 - loss: 0.0789 - val_accuracy: 0.9789 - val_loss: 0.0889\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9700 - loss: 0.0803 - val_accuracy: 0.9778 - val_loss: 0.0918\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9745 - loss: 0.0742 - val_accuracy: 0.9784 - val_loss: 0.0868\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9715 - loss: 0.0752 - val_accuracy: 0.9756 - val_loss: 0.0914\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9698 - loss: 0.0771 - val_accuracy: 0.9780 - val_loss: 0.0927\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9681 - loss: 0.0814 - val_accuracy: 0.9567 - val_loss: 0.0978\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.0743 - val_accuracy: 0.9784 - val_loss: 0.0915\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0834 - val_accuracy: 0.9780 - val_loss: 0.0843\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9756 - loss: 0.0672 - val_accuracy: 0.9769 - val_loss: 0.0861\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9720 - loss: 0.0735 - val_accuracy: 0.9767 - val_loss: 0.0893\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9740 - loss: 0.0682 - val_accuracy: 0.9778 - val_loss: 0.0846\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9758 - loss: 0.0667 - val_accuracy: 0.9767 - val_loss: 0.0945\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9747 - loss: 0.0656 - val_accuracy: 0.9780 - val_loss: 0.0913\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model1’s accuracy ranges between 96% and 97.5%.\nFor the second model, we will use only the article text as input. Once again, we’ll compile and then train the model.\n\ntrain_texts = train.map(lambda x, y: x['text'])\ntext_vectorize_layer.adapt(train_texts)\n\n# input Layer\ntext_input = keras.Input(shape=(1,), dtype=\"string\", name=\"text\")\n\n# text processing layers\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(text_features)\n\n# Model Compilation\nmodel2 = keras.Model(inputs=text_input, outputs=output)\nmodel2.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\nmodel2.summary()\n\nModel: \"functional_8\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_10 (Dropout)                 │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_13          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_11 (Dropout)                 │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 21ms/step - accuracy: 0.5335 - loss: 0.6873 - val_accuracy: 0.8627 - val_loss: 0.6162\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 42ms/step - accuracy: 0.8523 - loss: 0.5490 - val_accuracy: 0.9489 - val_loss: 0.3287\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9127 - loss: 0.3132 - val_accuracy: 0.9229 - val_loss: 0.2216\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9291 - loss: 0.2258 - val_accuracy: 0.9189 - val_loss: 0.1876\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9337 - loss: 0.1943 - val_accuracy: 0.9260 - val_loss: 0.1683\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9443 - loss: 0.1668 - val_accuracy: 0.9358 - val_loss: 0.1487\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9461 - loss: 0.1542 - val_accuracy: 0.9356 - val_loss: 0.1424\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9473 - loss: 0.1461 - val_accuracy: 0.9447 - val_loss: 0.1310\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9509 - loss: 0.1354 - val_accuracy: 0.9478 - val_loss: 0.1252\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9534 - loss: 0.1321 - val_accuracy: 0.9371 - val_loss: 0.1382\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9532 - loss: 0.1277 - val_accuracy: 0.9498 - val_loss: 0.1184\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1149 - val_accuracy: 0.9513 - val_loss: 0.1149\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9591 - loss: 0.1120 - val_accuracy: 0.9609 - val_loss: 0.1018\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9618 - loss: 0.1044 - val_accuracy: 0.9542 - val_loss: 0.1078\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9639 - loss: 0.1021 - val_accuracy: 0.9529 - val_loss: 0.1085\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9631 - loss: 0.0975 - val_accuracy: 0.9609 - val_loss: 0.0968\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9620 - loss: 0.0958 - val_accuracy: 0.9458 - val_loss: 0.1157\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1071 - val_accuracy: 0.9542 - val_loss: 0.1060\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9673 - loss: 0.0880 - val_accuracy: 0.9533 - val_loss: 0.1069\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9646 - loss: 0.0891 - val_accuracy: 0.9598 - val_loss: 0.0926\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9667 - loss: 0.0844 - val_accuracy: 0.9580 - val_loss: 0.0994\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9658 - loss: 0.0888 - val_accuracy: 0.9602 - val_loss: 0.0916\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9685 - loss: 0.0788 - val_accuracy: 0.9600 - val_loss: 0.0923\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9702 - loss: 0.0755 - val_accuracy: 0.9633 - val_loss: 0.0896\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0788 - val_accuracy: 0.9531 - val_loss: 0.1074\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9712 - loss: 0.0748 - val_accuracy: 0.9593 - val_loss: 0.0921\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9726 - loss: 0.0703 - val_accuracy: 0.9607 - val_loss: 0.0903\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9743 - loss: 0.0654 - val_accuracy: 0.9596 - val_loss: 0.0916\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9755 - loss: 0.0656 - val_accuracy: 0.9482 - val_loss: 0.1184\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model2’s accuracy ranges between 96.5% and 97.5%, which is a slight improvement compared to model1.\nFor the third model, we will use both the article title and the article text as input.\n\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Dropout, GlobalAveragePooling1D, Dense, Input, concatenate\n\ntrain_data = train.map(lambda x, y: (x[\"title\"], x[\"text\"]))\ntrain_titles, train_texts = zip(*train_data)\n\nshared_embedding = Embedding(size_vocabulary, 3, name = \"embedding\")\n\ntitle_input = Input(shape=(1,), dtype=tf.string, name=\"title\")\ntitle_vectorized = title_vectorize_layer(title_input)\ntitle_embedded = shared_embedding(title_vectorized)\ntitle_processed = GlobalAveragePooling1D()(title_embedded)\n\ntext_input = Input(shape=(1,), dtype=tf.string, name=\"text\")\ntext_vectorized = text_vectorize_layer(text_input)\ntext_embedded = shared_embedding(text_vectorized)\ntext_processed = GlobalAveragePooling1D()(text_embedded)\n\ncombined_features = concatenate([title_processed, text_processed])\noutput = Dense(1, activation='sigmoid')(combined_features)\n\nmodel3 = keras.Model(inputs=[title_input, text_input], outputs=output)\nmodel3.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel3.summary()\n\nModel: \"functional_11\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 3)         │          6,000 │ text_vectorization[7]… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[1][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_4             │ (None, 6)              │              0 │ global_average_poolin… │\n│ (Concatenate)             │                        │                │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_16 (Dense)          │ (None, 1)              │              7 │ concatenate_4[0][0]    │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 6,007 (23.46 KB)\n\n\n\n Trainable params: 6,007 (23.46 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.5798 - loss: 0.6851 - val_accuracy: 0.6607 - val_loss: 0.6560\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.6959 - loss: 0.6448 - val_accuracy: 0.7358 - val_loss: 0.6053\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.7639 - loss: 0.5922 - val_accuracy: 0.8273 - val_loss: 0.5465\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.8506 - loss: 0.5326 - val_accuracy: 0.9038 - val_loss: 0.4869\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9067 - loss: 0.4742 - val_accuracy: 0.9291 - val_loss: 0.4338\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9252 - loss: 0.4234 - val_accuracy: 0.9378 - val_loss: 0.3895\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9342 - loss: 0.3814 - val_accuracy: 0.9424 - val_loss: 0.3534\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 36ms/step - accuracy: 0.9386 - loss: 0.3473 - val_accuracy: 0.9447 - val_loss: 0.3241\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9416 - loss: 0.3195 - val_accuracy: 0.9464 - val_loss: 0.3000\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9448 - loss: 0.2966 - val_accuracy: 0.9473 - val_loss: 0.2800\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 30ms/step - accuracy: 0.9460 - loss: 0.2776 - val_accuracy: 0.9496 - val_loss: 0.2631\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9478 - loss: 0.2614 - val_accuracy: 0.9518 - val_loss: 0.2488\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9500 - loss: 0.2476 - val_accuracy: 0.9527 - val_loss: 0.2364\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9514 - loss: 0.2357 - val_accuracy: 0.9556 - val_loss: 0.2257\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9527 - loss: 0.2252 - val_accuracy: 0.9558 - val_loss: 0.2162\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9541 - loss: 0.2159 - val_accuracy: 0.9556 - val_loss: 0.2079\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9552 - loss: 0.2077 - val_accuracy: 0.9564 - val_loss: 0.2004\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9564 - loss: 0.2002 - val_accuracy: 0.9576 - val_loss: 0.1937\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9573 - loss: 0.1935 - val_accuracy: 0.9589 - val_loss: 0.1876\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9583 - loss: 0.1874 - val_accuracy: 0.9602 - val_loss: 0.1821\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9598 - loss: 0.1818 - val_accuracy: 0.9618 - val_loss: 0.1771\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1767 - val_accuracy: 0.9627 - val_loss: 0.1725\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9615 - loss: 0.1719 - val_accuracy: 0.9636 - val_loss: 0.1683\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 22ms/step - accuracy: 0.9618 - loss: 0.1675 - val_accuracy: 0.9638 - val_loss: 0.1644\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9626 - loss: 0.1634 - val_accuracy: 0.9642 - val_loss: 0.1607\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9631 - loss: 0.1596 - val_accuracy: 0.9649 - val_loss: 0.1574\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9638 - loss: 0.1560 - val_accuracy: 0.9651 - val_loss: 0.1542\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9641 - loss: 0.1526 - val_accuracy: 0.9667 - val_loss: 0.1513\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9655 - loss: 0.1495 - val_accuracy: 0.9669 - val_loss: 0.1485\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9658 - loss: 0.1464 - val_accuracy: 0.9680 - val_loss: 0.1460\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9665 - loss: 0.1436 - val_accuracy: 0.9682 - val_loss: 0.1435\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9668 - loss: 0.1409 - val_accuracy: 0.9684 - val_loss: 0.1412\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9677 - loss: 0.1383 - val_accuracy: 0.9693 - val_loss: 0.1391\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9681 - loss: 0.1359 - val_accuracy: 0.9696 - val_loss: 0.1370\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9682 - loss: 0.1335 - val_accuracy: 0.9702 - val_loss: 0.1351\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9689 - loss: 0.1313 - val_accuracy: 0.9700 - val_loss: 0.1332\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9690 - loss: 0.1291 - val_accuracy: 0.9702 - val_loss: 0.1315\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9694 - loss: 0.1271 - val_accuracy: 0.9711 - val_loss: 0.1298\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 38ms/step - accuracy: 0.9695 - loss: 0.1251 - val_accuracy: 0.9713 - val_loss: 0.1282\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 27ms/step - accuracy: 0.9697 - loss: 0.1232 - val_accuracy: 0.9718 - val_loss: 0.1267\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9700 - loss: 0.1213 - val_accuracy: 0.9727 - val_loss: 0.1252\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9701 - loss: 0.1196 - val_accuracy: 0.9729 - val_loss: 0.1238\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9707 - loss: 0.1178 - val_accuracy: 0.9727 - val_loss: 0.1224\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.1162 - val_accuracy: 0.9729 - val_loss: 0.1211\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9718 - loss: 0.1146 - val_accuracy: 0.9729 - val_loss: 0.1199\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9721 - loss: 0.1130 - val_accuracy: 0.9733 - val_loss: 0.1187\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9723 - loss: 0.1115 - val_accuracy: 0.9731 - val_loss: 0.1176\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 35ms/step - accuracy: 0.9725 - loss: 0.1100 - val_accuracy: 0.9736 - val_loss: 0.1164\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 24ms/step - accuracy: 0.9729 - loss: 0.1086 - val_accuracy: 0.9736 - val_loss: 0.1154\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9731 - loss: 0.1072 - val_accuracy: 0.9738 - val_loss: 0.1143\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nCompared to the previous two models, model3 appears to perform the best, because its accuracy ranges between 97% and 97.31% towards the last 10 epochs. Though it’s not as high as the other two models, model3 is able to consistently score at least 97% validation accuracy. Whereas the other two models often fluctuate between 96% and 97%."
  },
  {
    "objectID": "posts/hw6/index.html#model-evaluation",
    "href": "posts/hw6/index.html#model-evaluation",
    "title": "Sample",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\nWe will now use model3 to test my model performance on unseen data.\n\n# load test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(train_url)\ntest_data.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntest = make_dataset(test_data)\nloss, accuracy = model3.evaluate(test)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9737 - loss: 0.1090\nTest Loss: 0.10588612407445908\nTest Accuracy: 0.9754109382629395\n\n\nIt appears that model3 managed to get a test accuracy of 97.5%, which is great!"
  },
  {
    "objectID": "posts/hw6/index.html#embedding-visualization",
    "href": "posts/hw6/index.html#embedding-visualization",
    "title": "Sample",
    "section": "5. Embedding Visualization",
    "text": "5. Embedding Visualization\nLet’s visualize and comment on the embedding that model3 learned.\n\n# to display plotly plots on quarto\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nweights = model3.get_layer('embedding').get_weights()[0]\n\nvocab = title_vectorize_layer.get_vocabulary()\n\npca = PCA(n_components=2)\nreduced_weights = pca.fit_transform(weights)\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': reduced_weights[:, 0],\n    'x1': reduced_weights[:, 1]\n})\n\nfig = px.scatter(embedding_df,\n                 x=\"x0\",\n                 y=\"x1\",\n                 hover_name=\"word\",  # shows the word on hover\n                 title=\"2D PCA of Word Embeddings\",\n                 width=800,\n                 height=600)\nfig.update_traces(marker=dict(size=5,\n                              line=dict(width=1,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\nIn the graph above, the embedding of the words “Wednesday”, “Thursday”, “Monday”, “Tueday”, and “Friday” appear to be clustered together but not inside the central cluster. Moreover, their x1 values appear to be close to 0. Whereas their x0 values appear to range between -16 and -19. Since they are clustered outside of the central cluster, this means that these words likely do not help one differentiate between fake and real news.\nMoreover, the fact that they are clustered together also makes sense, because all of them are days of the week."
  },
  {
    "objectID": "posts/no-plan-pantry/index.html",
    "href": "posts/no-plan-pantry/index.html",
    "title": "No-plan Pantry",
    "section": "",
    "text": "Our project, “No-Plan Pantry”, aims to minimize food waste and inspire creativity in the kitchen by developing a web-application which generates recipes based on user-inputted ingredients. Using web scraping, recipes are collected from allrecipes.com and cleaned to ensure data quality. Advanced machine learning models, such as GPT-2 and BERT, are further employed to generate AI-based recipes and predict cooking times using textual recipe instructions. The application integrates these features with a user-friendly interface developed using Dash.\nData cleaning plays a critical role in standardizing recipe information, such as unifying time units, extracting key ingredients, and estimating missing cooking times. This step ensures the dataset is well-structured for machine learning models, enabling accurate recipe recommendations and cooking time predictions. The app’s interactivity allows users to select ingredients, set time or calorie preferences, and view both AI-generated and sourced recipes in an organized layout.\nOverall, the No-Plan Pantry project combines data science, natural language processing, and intuitive design to create an innovative tool that empowers users to cook creatively while reducing food waste.\nHere is the link to our GitHub repository.\n\n\n\nProject Overview"
  },
  {
    "objectID": "posts/no-plan-pantry/index.html#remarks",
    "href": "posts/no-plan-pantry/index.html#remarks",
    "title": "No-plan Pantry",
    "section": "Remarks",
    "text": "Remarks\nThe overall project is logically structured by integrating an advanced machine learning model, industry-standard frameworks, data analytics insights, neural network applications, and aesthetic design principles. Throughout the process, we encountered numerous challenges and discovered additional innovative ideas, many of which were successfully incorporated into the system. The AI-generated model, in particular, exceeded our initial expectations, providing enhanced functionality and value.\nHowever, there remains significant room for expansion and improvement. For instance, our web scraping efforts could be scaled to acquire a much larger and more diverse dataset, further enriching the system’s capabilities. Additionally, the configuration of the AI-generated model can be optimized for greater speed and efficiency, ensuring a smoother and faster user experience. These enhancements will not only strengthen the project’s current foundation but also open the door for further innovation and scalability in the future."
  },
  {
    "objectID": "posts/no-plan-pantry/index.html#ethical-ramifications",
    "href": "posts/no-plan-pantry/index.html#ethical-ramifications",
    "title": "No-plan Pantry",
    "section": "Ethical Ramifications",
    "text": "Ethical Ramifications\nThis project does raise important ethical considerations. For instance, web scraping recipes from allrecipes.com without explicit consent may violate terms of service. Proper care must be taken to respect copyright and intellectual property. Further, user privacy raises concern, since inputted data such as ingredients must be protected under laws like GDPR. AI-generated recipes may also produce unsafe or inaccurate instructions, requiring transparency in regards to their limitations. Bias in the dataset could favor certain cuisines, which reduces inclusivity. Finally, although this project aims to reduce food waste, it may lead to users purchasing more ingredients, potentially causing waste accumulation. To conclude, addressing these ethical issues is essential to ensuring the project’s benefits outweigh its risks."
  }
]