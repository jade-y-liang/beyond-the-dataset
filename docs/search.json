[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Detecting Fake vs. Real News",
    "section": "",
    "text": "In this post, I will show you how to develop and assess a fake news classifier using Keras."
  },
  {
    "objectID": "posts/index.html#create-models",
    "href": "posts/index.html#create-models",
    "title": "Detecting Fake vs. Real News",
    "section": "3. Create Models",
    "text": "3. Create Models\nWe will create three Keras model to offer a perspective the following question:\nWhen detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?\nFor the first model, we will use only the article title as input.\n\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\ntrain_data = train.map(lambda x, y: x[\"title\"])\ntitle_vectorize_layer.adapt(train_data)\n\n# input layer\ntitle_input = keras.Input(shape=(1,), dtype=\"string\", name=\"title\")\n\n# text processing layers\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(title_features)\n\n\n# model compilation (need to do this before training)\nmodel1 = keras.Model(inputs=title_input, outputs=output)\nmodel1.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel1.summary()\n\nModel: \"functional_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_8 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_12          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_9 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_10 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_11 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nfrom keras import utils\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\nNow we train our model.\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.5263 - loss: 0.6890 - val_accuracy: 0.8038 - val_loss: 0.6404\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 27ms/step - accuracy: 0.8175 - loss: 0.5799 - val_accuracy: 0.9451 - val_loss: 0.3571\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9034 - loss: 0.3341 - val_accuracy: 0.9218 - val_loss: 0.2310\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9200 - loss: 0.2399 - val_accuracy: 0.9253 - val_loss: 0.1896\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9359 - loss: 0.1982 - val_accuracy: 0.9187 - val_loss: 0.1785\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 33ms/step - accuracy: 0.9346 - loss: 0.1861 - val_accuracy: 0.9178 - val_loss: 0.1721\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 23ms/step - accuracy: 0.9419 - loss: 0.1732 - val_accuracy: 0.9304 - val_loss: 0.1544\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9456 - loss: 0.1605 - val_accuracy: 0.9420 - val_loss: 0.1401\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9461 - loss: 0.1510 - val_accuracy: 0.9318 - val_loss: 0.1473\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 31ms/step - accuracy: 0.9489 - loss: 0.1432 - val_accuracy: 0.9484 - val_loss: 0.1268\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9529 - loss: 0.1387 - val_accuracy: 0.9518 - val_loss: 0.1197\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9566 - loss: 0.1315 - val_accuracy: 0.9736 - val_loss: 0.1140\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9585 - loss: 0.1236 - val_accuracy: 0.9498 - val_loss: 0.1200\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1222 - val_accuracy: 0.9529 - val_loss: 0.1128\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 34ms/step - accuracy: 0.9593 - loss: 0.1132 - val_accuracy: 0.9458 - val_loss: 0.1232\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9578 - loss: 0.1207 - val_accuracy: 0.9504 - val_loss: 0.1169\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9614 - loss: 0.1091 - val_accuracy: 0.9544 - val_loss: 0.1068\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9612 - loss: 0.1090 - val_accuracy: 0.9549 - val_loss: 0.1049\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9593 - loss: 0.1082 - val_accuracy: 0.9564 - val_loss: 0.1032\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9630 - loss: 0.1005 - val_accuracy: 0.9551 - val_loss: 0.1044\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 32ms/step - accuracy: 0.9616 - loss: 0.1012 - val_accuracy: 0.9540 - val_loss: 0.1082\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9661 - loss: 0.0966 - val_accuracy: 0.9549 - val_loss: 0.1027\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9638 - loss: 0.0972 - val_accuracy: 0.9544 - val_loss: 0.1038\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9652 - loss: 0.0931 - val_accuracy: 0.9780 - val_loss: 0.0962\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9687 - loss: 0.0883 - val_accuracy: 0.9776 - val_loss: 0.0956\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 32ms/step - accuracy: 0.9664 - loss: 0.0908 - val_accuracy: 0.9553 - val_loss: 0.1004\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9662 - loss: 0.0882 - val_accuracy: 0.9769 - val_loss: 0.0926\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9686 - loss: 0.0857 - val_accuracy: 0.9776 - val_loss: 0.0880\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.9705 - loss: 0.0806 - val_accuracy: 0.9782 - val_loss: 0.0901\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9715 - loss: 0.0789 - val_accuracy: 0.9789 - val_loss: 0.0889\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9700 - loss: 0.0803 - val_accuracy: 0.9778 - val_loss: 0.0918\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9745 - loss: 0.0742 - val_accuracy: 0.9784 - val_loss: 0.0868\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9715 - loss: 0.0752 - val_accuracy: 0.9756 - val_loss: 0.0914\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9698 - loss: 0.0771 - val_accuracy: 0.9780 - val_loss: 0.0927\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9681 - loss: 0.0814 - val_accuracy: 0.9567 - val_loss: 0.0978\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.0743 - val_accuracy: 0.9784 - val_loss: 0.0915\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0834 - val_accuracy: 0.9780 - val_loss: 0.0843\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9756 - loss: 0.0672 - val_accuracy: 0.9769 - val_loss: 0.0861\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9720 - loss: 0.0735 - val_accuracy: 0.9767 - val_loss: 0.0893\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9740 - loss: 0.0682 - val_accuracy: 0.9778 - val_loss: 0.0846\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9758 - loss: 0.0667 - val_accuracy: 0.9767 - val_loss: 0.0945\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9747 - loss: 0.0656 - val_accuracy: 0.9780 - val_loss: 0.0913\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model1’s accuracy ranges between 96% and 97.5%.\nFor the second model, we will use only the article text as input. Once again, we’ll compile and then train the model.\n\ntrain_texts = train.map(lambda x, y: x['text'])\ntext_vectorize_layer.adapt(train_texts)\n\n# input Layer\ntext_input = keras.Input(shape=(1,), dtype=\"string\", name=\"text\")\n\n# text processing layers\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# output layer\noutput = layers.Dense(1, activation='sigmoid')(text_features)\n\n# Model Compilation\nmodel2 = keras.Model(inputs=text_input, outputs=output)\nmodel2.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\nmodel2.summary()\n\nModel: \"functional_8\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization_1                 │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding (Embedding)                │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_10 (Dropout)                 │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_13          │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_11 (Dropout)                 │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_12 (Dense)                     │ (None, 32)                  │             128 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_13 (Dense)                     │ (None, 1)                   │              33 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,161 (24.07 KB)\n\n\n\n Trainable params: 6,161 (24.07 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# to visualize the model\n\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model2.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 21ms/step - accuracy: 0.5335 - loss: 0.6873 - val_accuracy: 0.8627 - val_loss: 0.6162\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 42ms/step - accuracy: 0.8523 - loss: 0.5490 - val_accuracy: 0.9489 - val_loss: 0.3287\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9127 - loss: 0.3132 - val_accuracy: 0.9229 - val_loss: 0.2216\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9291 - loss: 0.2258 - val_accuracy: 0.9189 - val_loss: 0.1876\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9337 - loss: 0.1943 - val_accuracy: 0.9260 - val_loss: 0.1683\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9443 - loss: 0.1668 - val_accuracy: 0.9358 - val_loss: 0.1487\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 23ms/step - accuracy: 0.9461 - loss: 0.1542 - val_accuracy: 0.9356 - val_loss: 0.1424\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9473 - loss: 0.1461 - val_accuracy: 0.9447 - val_loss: 0.1310\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9509 - loss: 0.1354 - val_accuracy: 0.9478 - val_loss: 0.1252\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9534 - loss: 0.1321 - val_accuracy: 0.9371 - val_loss: 0.1382\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9532 - loss: 0.1277 - val_accuracy: 0.9498 - val_loss: 0.1184\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9564 - loss: 0.1149 - val_accuracy: 0.9513 - val_loss: 0.1149\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9591 - loss: 0.1120 - val_accuracy: 0.9609 - val_loss: 0.1018\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 21ms/step - accuracy: 0.9618 - loss: 0.1044 - val_accuracy: 0.9542 - val_loss: 0.1078\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9639 - loss: 0.1021 - val_accuracy: 0.9529 - val_loss: 0.1085\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9631 - loss: 0.0975 - val_accuracy: 0.9609 - val_loss: 0.0968\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9620 - loss: 0.0958 - val_accuracy: 0.9458 - val_loss: 0.1157\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1071 - val_accuracy: 0.9542 - val_loss: 0.1060\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.9673 - loss: 0.0880 - val_accuracy: 0.9533 - val_loss: 0.1069\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9646 - loss: 0.0891 - val_accuracy: 0.9598 - val_loss: 0.0926\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9667 - loss: 0.0844 - val_accuracy: 0.9580 - val_loss: 0.0994\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 29ms/step - accuracy: 0.9658 - loss: 0.0888 - val_accuracy: 0.9602 - val_loss: 0.0916\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9685 - loss: 0.0788 - val_accuracy: 0.9600 - val_loss: 0.0923\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9702 - loss: 0.0755 - val_accuracy: 0.9633 - val_loss: 0.0896\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9684 - loss: 0.0788 - val_accuracy: 0.9531 - val_loss: 0.1074\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 26ms/step - accuracy: 0.9712 - loss: 0.0748 - val_accuracy: 0.9593 - val_loss: 0.0921\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9726 - loss: 0.0703 - val_accuracy: 0.9607 - val_loss: 0.0903\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9743 - loss: 0.0654 - val_accuracy: 0.9596 - val_loss: 0.0916\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.9755 - loss: 0.0656 - val_accuracy: 0.9482 - val_loss: 0.1184\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nIt appears that model2’s accuracy ranges between 96.5% and 97.5%, which is a slight improvement compared to model1.\nFor the third model, we will use both the article title and the article text as input.\n\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Dropout, GlobalAveragePooling1D, Dense, Input, concatenate\n\ntrain_data = train.map(lambda x, y: (x[\"title\"], x[\"text\"]))\ntrain_titles, train_texts = zip(*train_data)\n\nshared_embedding = Embedding(size_vocabulary, 3, name = \"embedding\")\n\ntitle_input = Input(shape=(1,), dtype=tf.string, name=\"title\")\ntitle_vectorized = title_vectorize_layer(title_input)\ntitle_embedded = shared_embedding(title_vectorized)\ntitle_processed = GlobalAveragePooling1D()(title_embedded)\n\ntext_input = Input(shape=(1,), dtype=tf.string, name=\"text\")\ntext_vectorized = text_vectorize_layer(text_input)\ntext_embedded = shared_embedding(text_vectorized)\ntext_processed = GlobalAveragePooling1D()(text_embedded)\n\ncombined_features = concatenate([title_processed, text_processed])\noutput = Dense(1, activation='sigmoid')(combined_features)\n\nmodel3 = keras.Model(inputs=[title_input, text_input], outputs=output)\nmodel3.compile(optimizer='adam', loss='binary_crossentropy',\n                    metrics=['accuracy'])\n\nmodel3.summary()\n\nModel: \"functional_11\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0]            │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization_1      │ (None, 500)            │              0 │ text[0][0]             │\n│ (TextVectorization)       │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding (Embedding)     │ (None, 500, 3)         │          6,000 │ text_vectorization[7]… │\n│                           │                        │                │ text_vectorization_1[… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ embedding[1][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_4             │ (None, 6)              │              0 │ global_average_poolin… │\n│ (Concatenate)             │                        │                │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_16 (Dense)          │ (None, 1)              │              7 │ concatenate_4[0][0]    │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 6,007 (23.46 KB)\n\n\n\n Trainable params: 6,007 (23.46 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n# implement callback for early stopping to prevent overfitting\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n# train model\nhistory = model3.fit(train,\n                    validation_data=val,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.5798 - loss: 0.6851 - val_accuracy: 0.6607 - val_loss: 0.6560\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.6959 - loss: 0.6448 - val_accuracy: 0.7358 - val_loss: 0.6053\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.7639 - loss: 0.5922 - val_accuracy: 0.8273 - val_loss: 0.5465\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 27ms/step - accuracy: 0.8506 - loss: 0.5326 - val_accuracy: 0.9038 - val_loss: 0.4869\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9067 - loss: 0.4742 - val_accuracy: 0.9291 - val_loss: 0.4338\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 28ms/step - accuracy: 0.9252 - loss: 0.4234 - val_accuracy: 0.9378 - val_loss: 0.3895\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9342 - loss: 0.3814 - val_accuracy: 0.9424 - val_loss: 0.3534\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 36ms/step - accuracy: 0.9386 - loss: 0.3473 - val_accuracy: 0.9447 - val_loss: 0.3241\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9416 - loss: 0.3195 - val_accuracy: 0.9464 - val_loss: 0.3000\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9448 - loss: 0.2966 - val_accuracy: 0.9473 - val_loss: 0.2800\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 30ms/step - accuracy: 0.9460 - loss: 0.2776 - val_accuracy: 0.9496 - val_loss: 0.2631\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 22ms/step - accuracy: 0.9478 - loss: 0.2614 - val_accuracy: 0.9518 - val_loss: 0.2488\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 30ms/step - accuracy: 0.9500 - loss: 0.2476 - val_accuracy: 0.9527 - val_loss: 0.2364\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9514 - loss: 0.2357 - val_accuracy: 0.9556 - val_loss: 0.2257\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9527 - loss: 0.2252 - val_accuracy: 0.9558 - val_loss: 0.2162\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9541 - loss: 0.2159 - val_accuracy: 0.9556 - val_loss: 0.2079\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9552 - loss: 0.2077 - val_accuracy: 0.9564 - val_loss: 0.2004\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9564 - loss: 0.2002 - val_accuracy: 0.9576 - val_loss: 0.1937\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9573 - loss: 0.1935 - val_accuracy: 0.9589 - val_loss: 0.1876\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9583 - loss: 0.1874 - val_accuracy: 0.9602 - val_loss: 0.1821\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 24ms/step - accuracy: 0.9598 - loss: 0.1818 - val_accuracy: 0.9618 - val_loss: 0.1771\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9605 - loss: 0.1767 - val_accuracy: 0.9627 - val_loss: 0.1725\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9615 - loss: 0.1719 - val_accuracy: 0.9636 - val_loss: 0.1683\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 22ms/step - accuracy: 0.9618 - loss: 0.1675 - val_accuracy: 0.9638 - val_loss: 0.1644\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9626 - loss: 0.1634 - val_accuracy: 0.9642 - val_loss: 0.1607\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9631 - loss: 0.1596 - val_accuracy: 0.9649 - val_loss: 0.1574\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 20ms/step - accuracy: 0.9638 - loss: 0.1560 - val_accuracy: 0.9651 - val_loss: 0.1542\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 30ms/step - accuracy: 0.9641 - loss: 0.1526 - val_accuracy: 0.9667 - val_loss: 0.1513\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9655 - loss: 0.1495 - val_accuracy: 0.9669 - val_loss: 0.1485\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9658 - loss: 0.1464 - val_accuracy: 0.9680 - val_loss: 0.1460\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9665 - loss: 0.1436 - val_accuracy: 0.9682 - val_loss: 0.1435\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9668 - loss: 0.1409 - val_accuracy: 0.9684 - val_loss: 0.1412\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9677 - loss: 0.1383 - val_accuracy: 0.9693 - val_loss: 0.1391\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9681 - loss: 0.1359 - val_accuracy: 0.9696 - val_loss: 0.1370\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9682 - loss: 0.1335 - val_accuracy: 0.9702 - val_loss: 0.1351\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 25ms/step - accuracy: 0.9689 - loss: 0.1313 - val_accuracy: 0.9700 - val_loss: 0.1332\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9690 - loss: 0.1291 - val_accuracy: 0.9702 - val_loss: 0.1315\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9694 - loss: 0.1271 - val_accuracy: 0.9711 - val_loss: 0.1298\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 38ms/step - accuracy: 0.9695 - loss: 0.1251 - val_accuracy: 0.9713 - val_loss: 0.1282\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 27ms/step - accuracy: 0.9697 - loss: 0.1232 - val_accuracy: 0.9718 - val_loss: 0.1267\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9700 - loss: 0.1213 - val_accuracy: 0.9727 - val_loss: 0.1252\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 20ms/step - accuracy: 0.9701 - loss: 0.1196 - val_accuracy: 0.9729 - val_loss: 0.1238\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.9707 - loss: 0.1178 - val_accuracy: 0.9727 - val_loss: 0.1224\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9712 - loss: 0.1162 - val_accuracy: 0.9729 - val_loss: 0.1211\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9718 - loss: 0.1146 - val_accuracy: 0.9729 - val_loss: 0.1199\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 29ms/step - accuracy: 0.9721 - loss: 0.1130 - val_accuracy: 0.9733 - val_loss: 0.1187\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.9723 - loss: 0.1115 - val_accuracy: 0.9731 - val_loss: 0.1176\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 35ms/step - accuracy: 0.9725 - loss: 0.1100 - val_accuracy: 0.9736 - val_loss: 0.1164\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 8s 24ms/step - accuracy: 0.9729 - loss: 0.1086 - val_accuracy: 0.9736 - val_loss: 0.1154\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 34ms/step - accuracy: 0.9731 - loss: 0.1072 - val_accuracy: 0.9738 - val_loss: 0.1143\n\n\n\nplt.plot(history.history[\"accuracy\"],label='training')\nplt.plot(history.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(history.history[\"loss\"],label='training')\nplt.plot(history.history[\"val_loss\"],label='validation')\nplt.legend()\n\n\n\n\n\n\n\n\nCompared to the previous two models, model3 appears to perform the best, because its accuracy ranges between 97% and 97.31% towards the last 10 epochs. Though it’s not as high as the other two models, model3 is able to consistently score at least 97% validation accuracy. Whereas the other two models often fluctuate between 96% and 97%."
  },
  {
    "objectID": "posts/index.html#model-evaluation",
    "href": "posts/index.html#model-evaluation",
    "title": "Detecting Fake vs. Real News",
    "section": "4. Model Evaluation",
    "text": "4. Model Evaluation\nWe will now use model3 to test my model performance on unseen data.\n\n# load test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(train_url)\ntest_data.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntest = make_dataset(test_data)\nloss, accuracy = model3.evaluate(test)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9737 - loss: 0.1090\nTest Loss: 0.10588612407445908\nTest Accuracy: 0.9754109382629395\n\n\nIt appears that model3 managed to get a test accuracy of 97.5%, which is great!"
  },
  {
    "objectID": "posts/index.html#embedding-visualization",
    "href": "posts/index.html#embedding-visualization",
    "title": "Detecting Fake vs. Real News",
    "section": "5. Embedding Visualization",
    "text": "5. Embedding Visualization\nLet’s visualize and comment on the embedding that model3 learned.\n\n# to display plotly plots on quarto\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\nweights = model3.get_layer('embedding').get_weights()[0]\n\nvocab = title_vectorize_layer.get_vocabulary()\n\npca = PCA(n_components=2)\nreduced_weights = pca.fit_transform(weights)\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': reduced_weights[:, 0],\n    'x1': reduced_weights[:, 1]\n})\n\nfig = px.scatter(embedding_df,\n                 x=\"x0\",\n                 y=\"x1\",\n                 hover_name=\"word\",  # shows the word on hover\n                 title=\"2D PCA of Word Embeddings\",\n                 width=800,\n                 height=600)\nfig.update_traces(marker=dict(size=5,\n                              line=dict(width=1,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.show()\n\n\n\n\nIn the graph above, the embedding of the words “Wednesday”, “Thursday”, “Monday”, “Tueday”, and “Friday” appear to be clustered together but not inside the central cluster. Moreover, their x1 values appear to be close to 0. Whereas their x0 values appear to range between -16 and -19. Since they are clustered outside of the central cluster, this means that these words likely do not help one differentiate between fake and real news.\nMoreover, the fact that they are clustered together also makes sense, because all of them are days of the week."
  },
  {
    "objectID": "posts/hw1_data-wrangling-and-visualization/index.html",
    "href": "posts/hw1_data-wrangling-and-visualization/index.html",
    "title": "Creating Databases, Query Functions, and Interactive Plots",
    "section": "",
    "text": "Welcome! In this blog post, I’ll show you how to create a database, write a query function to access data from tables in a database, and create interesting visualizations using Plotly Express.\n\n1. Create a Database\nFirst, we’ll import the sqlite3 and pandas packages. Then, using sqlite3.connect(), we’ll create a database in our current directory called temps.db.\n\n# importing necessary package\nimport sqlite3\nimport pandas as pd\n\n# to create a database in the current directory called temps.db\nconn = sqlite3.connect(\"temps.db\") \n\nWe’ll be creating a new folder named datafiles, then downloading the temperatures datas into our new folder.\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files for the `temperatures` table\nimport urllib.request\nintervals = [f\"{10 * i + 1}-{10 * (i+1)}\" for i in range(190, 202)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/24F/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nNext, we’ll upload the tables temperatures, stations, and countries into our database. Note that there are some NaN values in the temperatures dataset. I’ve removed them using the prepare_df() function. Here’s what the prepare_df() looks like:\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n\n    # removing rows where Temp == NaN\n    df = df.dropna(subset = \"Temp\")\n\n    return df\n\n\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df) # prepares temperatures df and removes nan values\n    df.to_sql(\"temperatures\", conn, \n              if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nurl = \"https://raw.githubusercontent.com/PIC16B-ucla/24F/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\ncountries_url = \"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\"\ncountries = pd.read_csv(countries_url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\nLet’s check if our tables have been successfully uploaded into our database.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nLooks good! Now, after we’re done uploading tables to our database, let’s close our connection to our database using conn.close(). It’s generally a good practice to do this.\n\nconn.close()\n\n\n\n2. Write a Query Function\nNow that we have our tables in the database, let’s create a query function to allow easy access to our data in the database. I’ve created the function in… Here’s what the function looks like:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month) :\n    \"\"\" Extracts climate data from a specified database based on the provided country, \n        year range, and month; and returns extracted data in a Pandas dataframe\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n\n    Returns:\n        df (Pandas dataframe): a dataframe of temperature readings of inputted country according to \n            inputted year_begin, inputted year_end, and inputted month of the year. The\n            resulting dataframe contains the columns `NAME`, 'LATITUDE', 'LONGITUDE`, \n            `Country`, `Year`, `Month`, `Temp`.\n    \"\"\"\n\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(\"\"\"\n                    SELECT s.*, c.Name AS Country, t.*\n                    FROM stations s\n                    INNER JOIN countries c\n                    ON c.\"FIPS 10-4\" = SUBSTR(s.ID, 1, 2)\n                    INNER JOIN temperatures t\n                    ON t.ID = s.ID\n                    WHERE c.Name = ? AND t.Year &gt;= ? AND t.Year &lt;= ? AND t.Month = ?   \n                    ORDER BY NAME                            \n                    \"\"\", conn, params=(country, year_begin, year_end, month))\n        \n        # returns dataframe in order of columns mentioned before\n    return df[[\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\", \"Year\", \"Month\", \"Temp\"]]    \n\n\n\nThis is what the resulting dataframe looks like:\n\ndf = query_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\n3. Write a Geographic Scatter Function for Yearly Temperature Increases\nNow that we have easy access to certain subsets of our data, let’s visualize our data and see what we can learn from it. To do that, I’ll create a function that displays a geographic scatter plot, where eahc point shows the first coefficient of the linear regression model fitted by the temperatures each station in India from 1980 - 2020.\nTo evaluate the coefficients, we’ll need to implement a function that utilizes LinearRegression from sklearn.linear_model.\n\nfrom sklearn.linear_model import LinearRegression\ndef coef(data_group):\n    \"\"\" fits data to a linear regression model and outputs the first coefficient of the fitted model\n\n    Args:\n        data_group (Pandas dataframe): the data for which to fit to a model\n    \n    Returns:\n        \n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs) : \n    \"\"\"creates a geographic scatter plot where each point displays the first coefficent\n            of the linear regression model fitted for each station's temperature in given year range\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n        min_obs (integer): the minimum required number of years of data for any given station\n        **kwargs (optional): keyword arguments for the geographic scatter plot\n    \n    Returns:\n        an interactive geographic scatterplot\n    \"\"\"\n\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Filter out stations with observations less than min_obs\n    df['ObsCount'] = df.groupby('NAME')['NAME'].transform('count')\n    df = df[df['ObsCount'] &gt;= min_obs]\n\n    # Fitting each station data to a linear regression model\n    #   and put the first coefficient of the fitted models into a Pandas dataframe\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4).reset_index()\n\n    # Adding columns for latitude and longitude for each station in coefs\n    lat_lon = df[['NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates('NAME')\n    coefs = coefs.merge(lat_lon, how='left', left_on='NAME', right_on='NAME')\n    \n    # Creating dictionary to match month to month name (for title of plot)\n    month_dict={1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n            5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\",\n            10: \"October\", 11: \"November\", 12: \"December\"}\n    \n    # Preparing the plot\n    fig = px.scatter_mapbox(coefs,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=0,\n                            title = f\"Estimates of yearly increase in termperature in {month_dict[month]} &lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            **kwargs)\n    \n    # Updating margin and colorbar range\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}, \n                      coloraxis=dict(cmax=0.1, cmin=-0.1), # to make colorbar range from -0.1 to 0.1\n                      coloraxis_colorbar=dict(title='Estimated Yearly&lt;br&gt;Increase (°C)&lt;br&gt;')\n                      )\n    \n    return fig\n\n\n\n\nfrom plotly import express as px\n\ncolor_map = px.colors.sequential.RdBu_r\n\nmonth = 1\ncountry = \"India\"\nyear_begin = 1980\nyear_end = 2020\nfig = temperature_coefficient_plot(\"temps.db\",\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map,\n                                   width = 700)\n                                 \nfig.show()\n\n\n\n\nIt seems that a lot stations that surround the edges of India has a positive coefficient, which indicates that there are trends of increasing temperatures over the past decades. Notice how the stations in the central area of India yields negative coefficients.\nLet’s take a look at a geographic scatter plot for the U.S. during the same time period.\n\n\n\n\n\n\n\n4. Create Two More Interesting Figures\nLet’s see how the average annual temperatures has changed over the years.\n\nfrom climate_database import query_country_climate_database\ndf = query_country_climate_database(\"temps.db\", \"India\",1980, 2020)\ndf.head()\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.25\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.25\n1980\n3\n26.30\n\n\n2\nAGARTALA\n23.883\n91.25\n1980\n4\n29.72\n\n\n3\nAGARTALA\n23.883\n91.25\n1980\n5\n27.28\n\n\n4\nAGARTALA\n23.883\n91.25\n1980\n6\n28.56\n\n\n\n\n\n\n\nWe’ll be using the temperature_coefficient_plot, which I’ve created, to implement the heatmap.\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs) : \n    \"\"\"creates a geographic scatter plot where each point displays the first coefficent\n            of the linear regression model fitted for each station's temperature in given year range\n\n    Args:\n        db_file (string): file name for the database\n        country (string): name of the country for which data should be returned\n        year_begin (integer): the earliest year for which should be returned\n        year_end (integer): the latest years for which to should returned\n        month (integer): the month of the year for which should be returned\n        min_obs (integer): the minimum required number of years of data for any given station\n        **kwargs (optional): keyword arguments for the geographic scatter plot\n    \n    Returns:\n        an interactive geographic scatterplot\n    \"\"\"\n\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # Filter out stations with observations less than min_obs\n    df['ObsCount'] = df.groupby('NAME')['NAME'].transform('count')\n    df = df[df['ObsCount'] &gt;= min_obs]\n\n    # Fitting each station data to a linear regression model\n    #   and put the first coefficient of the fitted models into a Pandas dataframe\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef).round(4).reset_index()\n\n    # Adding columns for latitude and longitude for each station in coefs\n    lat_lon = df[['NAME', 'LATITUDE', 'LONGITUDE']].drop_duplicates('NAME')\n    coefs = coefs.merge(lat_lon, how='left', left_on='NAME', right_on='NAME')\n    \n    # Creating dictionary to match month to month name (for title of plot)\n    month_dict={1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n            5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\",\n            10: \"October\", 11: \"November\", 12: \"December\"}\n    \n    # Preparing the plot\n    fig = px.scatter_mapbox(coefs,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=0,\n                            title = f\"Estimates of yearly increase in termperature in {month_dict[month]} &lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            **kwargs)\n    \n    # Updating margin and colorbar range\n    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}, \n                      coloraxis=dict(cmax=0.1, cmin=-0.1), # to make colorbar range from -0.1 to 0.1\n                      coloraxis_colorbar=dict(title='Estimated Yearly&lt;br&gt;Increase (°C)&lt;br&gt;')\n                      )\n    \n    return fig\n\n\n\nBased on the heatmap below, it appears that, over the years, more and more stations have been a higher annual yearly temperature.\n\nfrom climate_database import yearly_avg_temp_heatmap\n\n# setting color map for heatmap\ncolor_map = px.colors.sequential.deep\n\nfig = yearly_avg_temp_heatmap(\"temps.db\", \"India\", 1980, 2020,\n                              color_continuous_scale = color_map)\n\nfig.show()\n\n\n\n\nWe can dig deeper to learn more about our data. Let’s see how the monthly averages in India has changed from 1980 to 2020, and see if there’s a particular month where the averages have increased.\n\nfrom climate_database import monthly_avg_temp_lineplots\n\nfig = monthly_avg_temp_lineplots(\"temps.db\", \"India\", 1980, 2020,\n                                 height = 1500,\n                                 width = 500)\nfig.show()\n\n\n\n\nWhile none of the plots above appear to have a strongly positive linear trend, we can see that, there’s a small positive linear trend in March and April, and from June to November. This indicates that the effects of climate change may be more apparent in those months in India."
  },
  {
    "objectID": "posts/no-plan-pantry/index.html",
    "href": "posts/no-plan-pantry/index.html",
    "title": "No-plan Pantry",
    "section": "",
    "text": "Our project, “No-Plan Pantry”, aims to minimize food waste and inspire creativity in the kitchen by developing a web-application which generates recipes based on user-inputted ingredients. Using web scraping, recipes are collected from allrecipes.com and cleaned to ensure data quality. Advanced machine learning models, such as GPT-2 and BERT, are further employed to generate AI-based recipes and predict cooking times using textual recipe instructions. The application integrates these features with a user-friendly interface developed using Dash.\nData cleaning plays a critical role in standardizing recipe information, such as unifying time units, extracting key ingredients, and estimating missing cooking times. This step ensures the dataset is well-structured for machine learning models, enabling accurate recipe recommendations and cooking time predictions. The app’s interactivity allows users to select ingredients, set time or calorie preferences, and view both AI-generated and sourced recipes in an organized layout.\nOverall, the No-Plan Pantry project combines data science, natural language processing, and intuitive design to create an innovative tool that empowers users to cook creatively while reducing food waste.\nHere is the link to our GitHub repository.\n\n\n\nProject Overview"
  },
  {
    "objectID": "posts/no-plan-pantry/index.html#remarks",
    "href": "posts/no-plan-pantry/index.html#remarks",
    "title": "No-plan Pantry",
    "section": "Remarks",
    "text": "Remarks\nThe overall project is logically structured by integrating an advanced machine learning model, industry-standard frameworks, data analytics insights, neural network applications, and aesthetic design principles. Throughout the process, we encountered numerous challenges and discovered additional innovative ideas, many of which were successfully incorporated into the system. The AI-generated model, in particular, exceeded our initial expectations, providing enhanced functionality and value.\nHowever, there remains significant room for expansion and improvement. For instance, our web scraping efforts could be scaled to acquire a much larger and more diverse dataset, further enriching the system’s capabilities. Additionally, the configuration of the AI-generated model can be optimized for greater speed and efficiency, ensuring a smoother and faster user experience. These enhancements will not only strengthen the project’s current foundation but also open the door for further innovation and scalability in the future."
  },
  {
    "objectID": "posts/no-plan-pantry/index.html#ethical-ramifications",
    "href": "posts/no-plan-pantry/index.html#ethical-ramifications",
    "title": "No-plan Pantry",
    "section": "Ethical Ramifications",
    "text": "Ethical Ramifications\nThis project does raise important ethical considerations. For instance, web scraping recipes from allrecipes.com without explicit consent may violate terms of service. Proper care must be taken to respect copyright and intellectual property. Further, user privacy raises concern, since inputted data such as ingredients must be protected under laws like GDPR. AI-generated recipes may also produce unsafe or inaccurate instructions, requiring transparency in regards to their limitations. Bias in the dataset could favor certain cuisines, which reduces inclusivity. Finally, although this project aims to reduce food waste, it may lead to users purchasing more ingredients, potentially causing waste accumulation. To conclude, addressing these ethical issues is essential to ensuring the project’s benefits outweigh its risks."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/hw3_webdev/index.html",
    "href": "posts/hw3_webdev/index.html",
    "title": "Web Development with Dash",
    "section": "",
    "text": "Welcome! In this post, I will go over how to create a webapp using Dash by Plotly. By the end of this blog, we’ll be able to create a website that accepts user input!"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-create-the-database-of-messages",
    "href": "posts/hw3_webdev/index.html#function-to-create-the-database-of-messages",
    "title": "Web Development with Dash",
    "section": "Function to Create the Database of Messages",
    "text": "Function to Create the Database of Messages\nWe’ll first make a function to set up a database to receive messages from users. This is what the function looks like:\n\nimport sqlite3\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"creates database of messages, creates a \n        `messages` table if not already in database,\n         and return the connection of message_db\n\n    Returns:\n        message_db: connection to the message_db database\n    \"\"\"\n    global message_db\n\n    if message_db is not None: # if database is not empty\n        return message_db\n    else:\n        # Connect to the database messages_db.sqlite\n        message_db = sqlite3.connect(\"messages_db.sqlite\", \n                                     check_same_thread=False)\n\n        # SQL command to create a `messages` table in \n        #   the database if it does not exist\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()     # saves changes\n        cursor.close()      # closes cursor\n\n        return message_db\n\n# setting up `message` table\nget_message_db()\n\n&lt;sqlite3.Connection at 0x120d30400&gt;"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-insert-a-users-message-into-database",
    "href": "posts/hw3_webdev/index.html#function-to-insert-a-users-message-into-database",
    "title": "Web Development with Dash",
    "section": "Function to Insert a User’s Message into Database",
    "text": "Function to Insert a User’s Message into Database\nNext, we’ll create a function that inserts a user’s inputted message into the database we just created.\n\ndef insert_message(handle, message):\n    \"\"\"\n    Inserts a new message into the database.\n    Args:\n        handle (str): The user handle.\n        message (str): The content of the message.\n    \"\"\"\n\n    # creating a cursor to our database\n    cur = message_db.cursor()\n    cur.execute(\"\"\"INSERT INTO messages (handle, message)\n                    VALUES (?, ?)\n                   \"\"\",\n                   (handle, message))\n    \n    # committing changes to enure row insertion is saved\n    message_db.commit()\n\n    cursor.close() # closing cursor\n    message_db.close()  # closing connection to database"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#callback-function-to-update-components",
    "href": "posts/hw3_webdev/index.html#callback-function-to-update-components",
    "title": "Web Development with Dash",
    "section": "Callback Function to Update Components",
    "text": "Callback Function to Update Components\nNow, we’ll create a callback function to update the components. Before doing so, we need to define our Dash app. Then we’ll implement the callback function submit() to allow user to submit their message along with their name or handle.\n\nimport dash\nfrom dash import html, dcc, Input, Output, State\n\n# creating Dash app\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    # page title\n    html.H1(\"A SIMPLE MESSAGE BANK\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0F807A'}),\n\n    # form for submission\n    html.H2(\"Submit\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Div(\"Your Message:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-message', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Div(\"Your Name or Handle:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-handle', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Button('Submit', id='submit-button',\n                style={'margin-top': '10px', \n                       'background-color': '#0B5551', \n                       'color': 'white', 'border': 'none'}),\n    html.Div(id='message-output'),\n\n    # to view submitted messages\n    html.H2(\"View\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Button('Update', id='refresh-button', \n                style={'background-color': '#4A708B', \n                       'color': 'white'}),\n    html.Div(id='messages-display')\n])\n\n# define callback function to handle form submission\n@app.callback(\n    Output('message-output', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('user-handle', 'value'), \n     State('user-message', 'value')],\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    if not handle or not message: # if user didn't input both\n        return 'Please enter both a handle and a message.'\n    try: \n        insert_message(handle, message) \n        return 'Thank you for submitting a message!'\n    except Exception as e: # return error message if failed to insert message\n        return f'An error occurred: {str(e)}'\n\ndf = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n# Run the app\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#function-to-fetch-random-messages",
    "href": "posts/hw3_webdev/index.html#function-to-fetch-random-messages",
    "title": "Web Development with Dash",
    "section": "Function to Fetch Random Messages",
    "text": "Function to Fetch Random Messages\nThen, we’ll create a function to fetch random messages in our database.\n\ndef random_messages(n):\n    \"\"\"\n    Fetches a random selection of messages from the database.\n    Args:\n        n (int): Number of random messages to retrieve.\n    Returns:\n        list of tuples: A list of messages with their handles.\n    \"\"\"\n\n    # connect to database\n    db = get_message_db()\n\n    # create a cursor\n    cursor = db.cursor()\n\n    # extracting n random messages\n    query = \"SELECT name_or_handle, message FROM messages ORDER BY RANDOM() LIMIT ?\"\n    cursor.execute(query, (n,))\n    messages = cursor.fetchall()\n\n    # close cursor and connection to database\n    cursor.close()\n    db.close()\n    return messages"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#callback-function-to-display-random-messages",
    "href": "posts/hw3_webdev/index.html#callback-function-to-display-random-messages",
    "title": "Web Development with Dash",
    "section": "Callback Function to Display Random Messages",
    "text": "Callback Function to Display Random Messages\n\n@app.callback(\n    Output('messages-display', 'children'),\n    Input('refresh-button', 'n_clicks'),\n    prevent_initial_call=True\n)\ndef view(n_clicks):\n    try:\n        messages = random_messages(5) # extracting 5 random messages\n        return [\n            # displays each message in one line\n            html.Div([\n                html.P(message, style={'font-size': '16px'}),\n                html.P(f\"- {handle}\", style={'font-size': '16px', 'text-align': 'right'})\n            ], style={'margin-bottom': '20px', 'border-bottom': '1px solid #ccc', 'padding-bottom': '10px'})\n            for handle, message in messages\n        ]\n    except Exception as e:\n        # return error message if failed to fetch messages\n        return html.Div(f\"Failed to fetch messages: {str(e)}\")"
  },
  {
    "objectID": "posts/hw3_webdev/index.html#to-view-final-results",
    "href": "posts/hw3_webdev/index.html#to-view-final-results",
    "title": "Web Development with Dash",
    "section": "To View Final Results",
    "text": "To View Final Results\nWe’ll compile all our functions, update our Dash app to display messages that were submitted, and try using our message bank!\n\nimport sqlite3\nimport dash\nfrom dash import html, dcc, Input, Output, State\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"creates database of messages, creates a `messages` table if not already in database,\n         and return the connection of message_db\n\n    Returns:\n        message_db: connection to the message_db database\n    \"\"\"\n    global message_db\n\n    if message_db is not None: # if database is not empty\n        return message_db\n    else:\n        # Connect to the database messages_db.sqlite\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n        # SQL command to create a `messages` table in the database if it does not exist\n        cmd = '''\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT NOT NULL,\n            message TEXT NOT NULL\n        )\n        '''\n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        message_db.commit()     # saves changes\n        cursor.close()      # closes cursor\n\n        return message_db\n\n# setting up `message` table\nget_message_db()\n\napp = dash.Dash(__name__)\n\napp.layout = html.Div([\n    # page title\n    html.H1(\"A SIMPLE MESSAGE BANK\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0F807A'}),\n\n    # form for submission\n    html.H2(\"Submit\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Div(\"Your Message:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-message', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Div(\"Your Name or Handle:\",\n             style={'font-family': 'Arial, sans-serif', \n                    'color': '#678D8B'}),\n    dcc.Input(id='user-handle', type='text',\n              style={'width': '50%', 'margin-bottom': '5px'}),\n    html.Button('Submit', id='submit-button',\n                style={'margin-top': '10px', \n                       'background-color': '#0B5551', \n                       'color': 'white', 'border': 'none'}),\n    html.Div(id='message-output'),\n\n    # to view submitted messages\n    html.H2(\"View\",\n            style = {'font-family': 'Arial, sans-serif', \n                     'color': '#0B5551'}),\n    html.Button('Update', id='refresh-button', \n                style={'background-color': '#4A708B', \n                       'color': 'white'}),\n    html.Div(id='messages-display')\n])\n\ndef insert_message(handle, message):\n    with sqlite3.connect(\"messages_db.sqlite\") as conn:\n        cur = conn.cursor()\n        cur.execute(\"\"\"INSERT INTO messages \n                    (handle, message) \n                    VALUES (?, ?)\"\"\", \n                    (handle, message))\n        conn.commit()\n\ndef random_messages(n):\n    with sqlite3.connect(\"messages_db.sqlite\") as conn:\n        cur = conn.cursor()\n        cur.execute(\"\"\"SELECT handle, message FROM \n                    messages ORDER BY RANDOM() \n                    LIMIT ?\"\"\", (n,))\n        messages = cur.fetchall()\n    return messages\n\n@app.callback(\n    Output('message-output', 'children'),\n    Input('submit-button', 'n_clicks'),\n    [State('user-handle', 'value'), \n     State('user-message', 'value')],\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    if not handle or not message:\n        return 'Please enter both a handle and a message.'\n    try:\n        insert_message(handle, message)\n        return 'Thank you for submitting a message!'\n    except Exception as e:\n        return f'An error occurred: {str(e)}'\n\n@app.callback(\n    Output('messages-display', 'children'),\n    Input('refresh-button', 'n_clicks'),\n    prevent_initial_call=True\n)\ndef view(n_clicks):\n    try:\n        messages = random_messages(5) # extracting 5 random messages\n        return [\n            # displays each message in one line\n            html.Div([\n                html.P(message, style={'font-size': '16px'}),\n                html.P(f\"- {handle}\", \n                       style={'font-size': '16px', \n                              'text-align': 'right'})\n            ], style={'margin-bottom': '20px', \n                      'border-bottom': '1px solid #ccc', \n                      'padding-bottom': '10px'})\n            for handle, message in messages\n        ]\n    except Exception as e:\n        # return error message if failed to fetch messages\n        return html.Div(f\"Failed to fetch messages: {str(e)}\")\n\nif __name__ == '__main__':\n    app.run_server(debug=True)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome! :D\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/hw0_constructing-visualization-of-penguin-data/index.html",
    "href": "posts/hw0_constructing-visualization-of-penguin-data/index.html",
    "title": "How to Construct a Visuaization of the Palmer Penguins Data Set",
    "section": "",
    "text": "1. Load the Data Set\nBefore we can create visualizations for the Palmer Penguins Data set, we need to first import the data set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nHere’s the first five rows of the Palmer Penguins data set:\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n2. Import the Seaborn Package and Create a Visualization\nAfter importing the dataset, we can then import the seaborn package. Then, use seaborn.relplot() from the seaborn package to create a scatter plot that compares the body mass (g) to the flipper length (mm) of each penguin for each sex. Notice that there is a 3rd parameter for Sex where Sex = \".\". This is because there is one entry in the Palmer Penguins data set where the sex of that penguin isn’t specified.\n\nimport seaborn as sns\n\nfgrid = sns.relplot(x = \"Body Mass (g)\", \n                    y = \"Flipper Length (mm)\",\n                    hue = \"Sex\", # to color each point by Sex\n                    data = penguins\n                    )\n\nfgrid.fig.suptitle(\"Body Mass (g) vs. Flipper Length (mm)\") # to add title to plot\nfgrid.fig.subplots_adjust(top=0.9) # to adjust placement of title\n\n\n\n\n\n\n\n\nAnd that’s how you can create a simple scatter plot using Seaborn! You can adjust the arguments of sns.relplot() to create different scatter plots using Palmer Penguins data set."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "No-plan Pantry\n\n\n\n\n\n\nwebscraping\n\n\nmachine learning\n\n\nweb development\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nJade Liang, Junying Li, Karis Choi\n\n\n\n\n\n\n\n\n\n\n\n\nDetecting Fake vs. Real News\n\n\n\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Development with Dash\n\n\n\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Databases, Query Functions, and Interactive Plots\n\n\n\n\n\n\npic16b homework\n\n\nvisualizations\n\n\ndatabases\n\n\ninteractive graphics\n\n\nquery\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Construct a Visuaization of the Palmer Penguins Data Set\n\n\n\n\n\n\npic16B homework\n\n\ntutorial\n\n\nvisualizations\n\n\n\n\n\n\n\n\n\nOct 14, 2024\n\n\nJade Liang\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nJade Liang\n\n\n\n\n\n\nNo matching items"
  }
]